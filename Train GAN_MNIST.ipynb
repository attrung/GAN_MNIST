{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = [1,28,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(100, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "os.makedirs(\"./data/mnist\", exist_ok=True)\n",
    "os.makedirs(\"./images\", exist_ok = True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(28), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size= 128,\n",
    "    shuffle=True,\n",
    "    num_workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/469] [D loss: 0.724849] [G loss: 0.696622]\n",
      "[Epoch 0/200] [Batch 1/469] [D loss: 0.630939] [G loss: 0.693587]\n",
      "[Epoch 0/200] [Batch 2/469] [D loss: 0.561310] [G loss: 0.690880]\n",
      "[Epoch 0/200] [Batch 3/469] [D loss: 0.505745] [G loss: 0.688213]\n",
      "[Epoch 0/200] [Batch 4/469] [D loss: 0.461377] [G loss: 0.684848]\n",
      "[Epoch 0/200] [Batch 5/469] [D loss: 0.429889] [G loss: 0.680114]\n",
      "[Epoch 0/200] [Batch 6/469] [D loss: 0.408132] [G loss: 0.674331]\n",
      "[Epoch 0/200] [Batch 7/469] [D loss: 0.395487] [G loss: 0.667583]\n",
      "[Epoch 0/200] [Batch 8/469] [D loss: 0.389711] [G loss: 0.658517]\n",
      "[Epoch 0/200] [Batch 9/469] [D loss: 0.388282] [G loss: 0.648758]\n",
      "[Epoch 0/200] [Batch 10/469] [D loss: 0.392770] [G loss: 0.637124]\n",
      "[Epoch 0/200] [Batch 11/469] [D loss: 0.398350] [G loss: 0.623930]\n",
      "[Epoch 0/200] [Batch 12/469] [D loss: 0.403310] [G loss: 0.610474]\n",
      "[Epoch 0/200] [Batch 13/469] [D loss: 0.409586] [G loss: 0.599781]\n",
      "[Epoch 0/200] [Batch 14/469] [D loss: 0.416483] [G loss: 0.588464]\n",
      "[Epoch 0/200] [Batch 15/469] [D loss: 0.421845] [G loss: 0.582009]\n",
      "[Epoch 0/200] [Batch 16/469] [D loss: 0.426183] [G loss: 0.578757]\n",
      "[Epoch 0/200] [Batch 17/469] [D loss: 0.426448] [G loss: 0.582069]\n",
      "[Epoch 0/200] [Batch 18/469] [D loss: 0.430604] [G loss: 0.582525]\n",
      "[Epoch 0/200] [Batch 19/469] [D loss: 0.428388] [G loss: 0.592420]\n",
      "[Epoch 0/200] [Batch 20/469] [D loss: 0.427817] [G loss: 0.603513]\n",
      "[Epoch 0/200] [Batch 21/469] [D loss: 0.431706] [G loss: 0.607402]\n",
      "[Epoch 0/200] [Batch 22/469] [D loss: 0.427523] [G loss: 0.624341]\n",
      "[Epoch 0/200] [Batch 23/469] [D loss: 0.429118] [G loss: 0.628065]\n",
      "[Epoch 0/200] [Batch 24/469] [D loss: 0.434295] [G loss: 0.624606]\n",
      "[Epoch 0/200] [Batch 25/469] [D loss: 0.443847] [G loss: 0.618794]\n",
      "[Epoch 0/200] [Batch 26/469] [D loss: 0.450155] [G loss: 0.618923]\n",
      "[Epoch 0/200] [Batch 27/469] [D loss: 0.461673] [G loss: 0.603857]\n",
      "[Epoch 0/200] [Batch 28/469] [D loss: 0.477863] [G loss: 0.597734]\n",
      "[Epoch 0/200] [Batch 29/469] [D loss: 0.493365] [G loss: 0.560097]\n",
      "[Epoch 0/200] [Batch 30/469] [D loss: 0.502547] [G loss: 0.590916]\n",
      "[Epoch 0/200] [Batch 31/469] [D loss: 0.507951] [G loss: 0.562348]\n",
      "[Epoch 0/200] [Batch 32/469] [D loss: 0.510650] [G loss: 0.581179]\n",
      "[Epoch 0/200] [Batch 33/469] [D loss: 0.502636] [G loss: 0.579990]\n",
      "[Epoch 0/200] [Batch 34/469] [D loss: 0.492337] [G loss: 0.607437]\n",
      "[Epoch 0/200] [Batch 35/469] [D loss: 0.480649] [G loss: 0.617248]\n",
      "[Epoch 0/200] [Batch 36/469] [D loss: 0.474648] [G loss: 0.662860]\n",
      "[Epoch 0/200] [Batch 37/469] [D loss: 0.471007] [G loss: 0.617401]\n",
      "[Epoch 0/200] [Batch 38/469] [D loss: 0.469119] [G loss: 0.660644]\n",
      "[Epoch 0/200] [Batch 39/469] [D loss: 0.481469] [G loss: 0.686426]\n",
      "[Epoch 0/200] [Batch 40/469] [D loss: 0.510962] [G loss: 0.572260]\n",
      "[Epoch 0/200] [Batch 41/469] [D loss: 0.526009] [G loss: 0.611873]\n",
      "[Epoch 0/200] [Batch 42/469] [D loss: 0.548531] [G loss: 0.603836]\n",
      "[Epoch 0/200] [Batch 43/469] [D loss: 0.550627] [G loss: 0.534878]\n",
      "[Epoch 0/200] [Batch 44/469] [D loss: 0.588726] [G loss: 0.616868]\n",
      "[Epoch 0/200] [Batch 45/469] [D loss: 0.602489] [G loss: 0.482009]\n",
      "[Epoch 0/200] [Batch 46/469] [D loss: 0.587109] [G loss: 0.600070]\n",
      "[Epoch 0/200] [Batch 47/469] [D loss: 0.564959] [G loss: 0.589759]\n",
      "[Epoch 0/200] [Batch 48/469] [D loss: 0.562529] [G loss: 0.610806]\n",
      "[Epoch 0/200] [Batch 49/469] [D loss: 0.539137] [G loss: 0.655645]\n",
      "[Epoch 0/200] [Batch 50/469] [D loss: 0.534813] [G loss: 0.651171]\n",
      "[Epoch 0/200] [Batch 51/469] [D loss: 0.508261] [G loss: 0.729280]\n",
      "[Epoch 0/200] [Batch 52/469] [D loss: 0.498236] [G loss: 0.691075]\n",
      "[Epoch 0/200] [Batch 53/469] [D loss: 0.487981] [G loss: 0.818901]\n",
      "[Epoch 0/200] [Batch 54/469] [D loss: 0.495804] [G loss: 0.644098]\n",
      "[Epoch 0/200] [Batch 55/469] [D loss: 0.463836] [G loss: 1.026152]\n",
      "[Epoch 0/200] [Batch 56/469] [D loss: 0.569435] [G loss: 0.473994]\n",
      "[Epoch 0/200] [Batch 57/469] [D loss: 0.503042] [G loss: 0.937800]\n",
      "[Epoch 0/200] [Batch 58/469] [D loss: 0.560474] [G loss: 0.607276]\n",
      "[Epoch 0/200] [Batch 59/469] [D loss: 0.530769] [G loss: 0.651626]\n",
      "[Epoch 0/200] [Batch 60/469] [D loss: 0.545962] [G loss: 0.792326]\n",
      "[Epoch 0/200] [Batch 61/469] [D loss: 0.584243] [G loss: 0.517848]\n",
      "[Epoch 0/200] [Batch 62/469] [D loss: 0.553043] [G loss: 0.837900]\n",
      "[Epoch 0/200] [Batch 63/469] [D loss: 0.569542] [G loss: 0.551998]\n",
      "[Epoch 0/200] [Batch 64/469] [D loss: 0.552106] [G loss: 0.811726]\n",
      "[Epoch 0/200] [Batch 65/469] [D loss: 0.554579] [G loss: 0.558136]\n",
      "[Epoch 0/200] [Batch 66/469] [D loss: 0.475960] [G loss: 0.870256]\n",
      "[Epoch 0/200] [Batch 67/469] [D loss: 0.477836] [G loss: 0.754943]\n",
      "[Epoch 0/200] [Batch 68/469] [D loss: 0.428293] [G loss: 0.796076]\n",
      "[Epoch 0/200] [Batch 69/469] [D loss: 0.400910] [G loss: 1.005571]\n",
      "[Epoch 0/200] [Batch 70/469] [D loss: 0.417905] [G loss: 0.774250]\n",
      "[Epoch 0/200] [Batch 71/469] [D loss: 0.382345] [G loss: 1.020926]\n",
      "[Epoch 0/200] [Batch 72/469] [D loss: 0.404486] [G loss: 0.871562]\n",
      "[Epoch 0/200] [Batch 73/469] [D loss: 0.420519] [G loss: 0.882398]\n",
      "[Epoch 0/200] [Batch 74/469] [D loss: 0.462666] [G loss: 0.814613]\n",
      "[Epoch 0/200] [Batch 75/469] [D loss: 0.439882] [G loss: 0.840925]\n",
      "[Epoch 0/200] [Batch 76/469] [D loss: 0.464747] [G loss: 0.789077]\n",
      "[Epoch 0/200] [Batch 77/469] [D loss: 0.465917] [G loss: 0.858427]\n",
      "[Epoch 0/200] [Batch 78/469] [D loss: 0.475500] [G loss: 0.712349]\n",
      "[Epoch 0/200] [Batch 79/469] [D loss: 0.565354] [G loss: 0.799948]\n",
      "[Epoch 0/200] [Batch 80/469] [D loss: 0.582838] [G loss: 0.523005]\n",
      "[Epoch 0/200] [Batch 81/469] [D loss: 0.509279] [G loss: 0.897485]\n",
      "[Epoch 0/200] [Batch 82/469] [D loss: 0.510287] [G loss: 0.623609]\n",
      "[Epoch 0/200] [Batch 83/469] [D loss: 0.459886] [G loss: 0.838111]\n",
      "[Epoch 0/200] [Batch 84/469] [D loss: 0.468876] [G loss: 0.744043]\n",
      "[Epoch 0/200] [Batch 85/469] [D loss: 0.449798] [G loss: 0.755818]\n",
      "[Epoch 0/200] [Batch 86/469] [D loss: 0.427726] [G loss: 0.897296]\n",
      "[Epoch 0/200] [Batch 87/469] [D loss: 0.444978] [G loss: 0.723585]\n",
      "[Epoch 0/200] [Batch 88/469] [D loss: 0.415715] [G loss: 0.903403]\n",
      "[Epoch 0/200] [Batch 89/469] [D loss: 0.399416] [G loss: 0.804818]\n",
      "[Epoch 0/200] [Batch 90/469] [D loss: 0.370844] [G loss: 0.924867]\n",
      "[Epoch 0/200] [Batch 91/469] [D loss: 0.361400] [G loss: 0.938605]\n",
      "[Epoch 0/200] [Batch 92/469] [D loss: 0.350918] [G loss: 0.909120]\n",
      "[Epoch 0/200] [Batch 93/469] [D loss: 0.321487] [G loss: 1.001651]\n",
      "[Epoch 0/200] [Batch 94/469] [D loss: 0.339073] [G loss: 1.030669]\n",
      "[Epoch 0/200] [Batch 95/469] [D loss: 0.318730] [G loss: 0.903363]\n",
      "[Epoch 0/200] [Batch 96/469] [D loss: 0.339651] [G loss: 1.268289]\n",
      "[Epoch 0/200] [Batch 97/469] [D loss: 0.334433] [G loss: 0.844625]\n",
      "[Epoch 0/200] [Batch 98/469] [D loss: 0.291087] [G loss: 1.134965]\n",
      "[Epoch 0/200] [Batch 99/469] [D loss: 0.292727] [G loss: 1.224340]\n",
      "[Epoch 0/200] [Batch 100/469] [D loss: 0.327221] [G loss: 0.941923]\n",
      "[Epoch 0/200] [Batch 101/469] [D loss: 0.335757] [G loss: 1.179329]\n",
      "[Epoch 0/200] [Batch 102/469] [D loss: 0.361905] [G loss: 0.856763]\n",
      "[Epoch 0/200] [Batch 103/469] [D loss: 0.330276] [G loss: 1.024837]\n",
      "[Epoch 0/200] [Batch 104/469] [D loss: 0.393515] [G loss: 0.961756]\n",
      "[Epoch 0/200] [Batch 105/469] [D loss: 0.422685] [G loss: 0.741737]\n",
      "[Epoch 0/200] [Batch 106/469] [D loss: 0.436127] [G loss: 0.943518]\n",
      "[Epoch 0/200] [Batch 107/469] [D loss: 0.460107] [G loss: 0.876951]\n",
      "[Epoch 0/200] [Batch 108/469] [D loss: 0.473392] [G loss: 0.760715]\n",
      "[Epoch 0/200] [Batch 109/469] [D loss: 0.484724] [G loss: 1.022766]\n",
      "[Epoch 0/200] [Batch 110/469] [D loss: 0.487264] [G loss: 0.745213]\n",
      "[Epoch 0/200] [Batch 111/469] [D loss: 0.437816] [G loss: 1.083093]\n",
      "[Epoch 0/200] [Batch 112/469] [D loss: 0.434938] [G loss: 1.035303]\n",
      "[Epoch 0/200] [Batch 113/469] [D loss: 0.429066] [G loss: 0.920306]\n",
      "[Epoch 0/200] [Batch 114/469] [D loss: 0.430828] [G loss: 1.101390]\n",
      "[Epoch 0/200] [Batch 115/469] [D loss: 0.450081] [G loss: 0.933467]\n",
      "[Epoch 0/200] [Batch 116/469] [D loss: 0.454351] [G loss: 0.981491]\n",
      "[Epoch 0/200] [Batch 117/469] [D loss: 0.444584] [G loss: 0.969947]\n",
      "[Epoch 0/200] [Batch 118/469] [D loss: 0.463449] [G loss: 0.995281]\n",
      "[Epoch 0/200] [Batch 119/469] [D loss: 0.464032] [G loss: 0.872229]\n",
      "[Epoch 0/200] [Batch 120/469] [D loss: 0.481442] [G loss: 1.286273]\n",
      "[Epoch 0/200] [Batch 121/469] [D loss: 0.523145] [G loss: 0.651048]\n",
      "[Epoch 0/200] [Batch 122/469] [D loss: 0.481653] [G loss: 1.328711]\n",
      "[Epoch 0/200] [Batch 123/469] [D loss: 0.493348] [G loss: 0.870216]\n",
      "[Epoch 0/200] [Batch 124/469] [D loss: 0.479723] [G loss: 1.144484]\n",
      "[Epoch 0/200] [Batch 125/469] [D loss: 0.469674] [G loss: 0.967979]\n",
      "[Epoch 0/200] [Batch 126/469] [D loss: 0.481356] [G loss: 1.131743]\n",
      "[Epoch 0/200] [Batch 127/469] [D loss: 0.495622] [G loss: 0.899447]\n",
      "[Epoch 0/200] [Batch 128/469] [D loss: 0.512147] [G loss: 1.005180]\n",
      "[Epoch 0/200] [Batch 129/469] [D loss: 0.521460] [G loss: 0.837986]\n",
      "[Epoch 0/200] [Batch 130/469] [D loss: 0.532687] [G loss: 0.950427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 131/469] [D loss: 0.560447] [G loss: 0.705321]\n",
      "[Epoch 0/200] [Batch 132/469] [D loss: 0.581612] [G loss: 1.127679]\n",
      "[Epoch 0/200] [Batch 133/469] [D loss: 0.669071] [G loss: 0.455952]\n",
      "[Epoch 0/200] [Batch 134/469] [D loss: 0.610622] [G loss: 1.317782]\n",
      "[Epoch 0/200] [Batch 135/469] [D loss: 0.601936] [G loss: 0.566389]\n",
      "[Epoch 0/200] [Batch 136/469] [D loss: 0.500685] [G loss: 1.145540]\n",
      "[Epoch 0/200] [Batch 137/469] [D loss: 0.483496] [G loss: 1.041122]\n",
      "[Epoch 0/200] [Batch 138/469] [D loss: 0.470790] [G loss: 0.925278]\n",
      "[Epoch 0/200] [Batch 139/469] [D loss: 0.494365] [G loss: 1.030829]\n",
      "[Epoch 0/200] [Batch 140/469] [D loss: 0.528993] [G loss: 0.785312]\n",
      "[Epoch 0/200] [Batch 141/469] [D loss: 0.554385] [G loss: 1.053295]\n",
      "[Epoch 0/200] [Batch 142/469] [D loss: 0.627359] [G loss: 0.539959]\n",
      "[Epoch 0/200] [Batch 143/469] [D loss: 0.632106] [G loss: 1.094083]\n",
      "[Epoch 0/200] [Batch 144/469] [D loss: 0.743012] [G loss: 0.349513]\n",
      "[Epoch 0/200] [Batch 145/469] [D loss: 0.680841] [G loss: 1.139556]\n",
      "[Epoch 0/200] [Batch 146/469] [D loss: 0.658750] [G loss: 0.461595]\n",
      "[Epoch 0/200] [Batch 147/469] [D loss: 0.620682] [G loss: 0.909343]\n",
      "[Epoch 0/200] [Batch 148/469] [D loss: 0.584445] [G loss: 0.724666]\n",
      "[Epoch 0/200] [Batch 149/469] [D loss: 0.587950] [G loss: 0.738778]\n",
      "[Epoch 0/200] [Batch 150/469] [D loss: 0.596356] [G loss: 0.873705]\n",
      "[Epoch 0/200] [Batch 151/469] [D loss: 0.613422] [G loss: 0.622490]\n",
      "[Epoch 0/200] [Batch 152/469] [D loss: 0.581630] [G loss: 0.859888]\n",
      "[Epoch 0/200] [Batch 153/469] [D loss: 0.622884] [G loss: 0.670804]\n",
      "[Epoch 0/200] [Batch 154/469] [D loss: 0.582195] [G loss: 0.808565]\n",
      "[Epoch 0/200] [Batch 155/469] [D loss: 0.577633] [G loss: 0.836179]\n",
      "[Epoch 0/200] [Batch 156/469] [D loss: 0.580372] [G loss: 0.744606]\n",
      "[Epoch 0/200] [Batch 157/469] [D loss: 0.601308] [G loss: 0.924632]\n",
      "[Epoch 0/200] [Batch 158/469] [D loss: 0.655701] [G loss: 0.529964]\n",
      "[Epoch 0/200] [Batch 159/469] [D loss: 0.653255] [G loss: 1.054079]\n",
      "[Epoch 0/200] [Batch 160/469] [D loss: 0.679748] [G loss: 0.422387]\n",
      "[Epoch 0/200] [Batch 161/469] [D loss: 0.621142] [G loss: 0.971426]\n",
      "[Epoch 0/200] [Batch 162/469] [D loss: 0.619935] [G loss: 0.726789]\n",
      "[Epoch 0/200] [Batch 163/469] [D loss: 0.596724] [G loss: 0.856850]\n",
      "[Epoch 0/200] [Batch 164/469] [D loss: 0.589871] [G loss: 0.825916]\n",
      "[Epoch 0/200] [Batch 165/469] [D loss: 0.636000] [G loss: 0.762828]\n",
      "[Epoch 0/200] [Batch 166/469] [D loss: 0.636431] [G loss: 0.670579]\n",
      "[Epoch 0/200] [Batch 167/469] [D loss: 0.635709] [G loss: 0.853423]\n",
      "[Epoch 0/200] [Batch 168/469] [D loss: 0.643293] [G loss: 0.562596]\n",
      "[Epoch 0/200] [Batch 169/469] [D loss: 0.598498] [G loss: 1.102886]\n",
      "[Epoch 0/200] [Batch 170/469] [D loss: 0.616170] [G loss: 0.539863]\n",
      "[Epoch 0/200] [Batch 171/469] [D loss: 0.559642] [G loss: 1.061148]\n",
      "[Epoch 0/200] [Batch 172/469] [D loss: 0.511956] [G loss: 0.689611]\n",
      "[Epoch 0/200] [Batch 173/469] [D loss: 0.481913] [G loss: 1.140188]\n",
      "[Epoch 0/200] [Batch 174/469] [D loss: 0.526256] [G loss: 0.703520]\n",
      "[Epoch 0/200] [Batch 175/469] [D loss: 0.516737] [G loss: 1.164370]\n",
      "[Epoch 0/200] [Batch 176/469] [D loss: 0.624695] [G loss: 0.604045]\n",
      "[Epoch 0/200] [Batch 177/469] [D loss: 0.644655] [G loss: 0.945228]\n",
      "[Epoch 0/200] [Batch 178/469] [D loss: 0.705963] [G loss: 0.531845]\n",
      "[Epoch 0/200] [Batch 179/469] [D loss: 0.657541] [G loss: 0.897641]\n",
      "[Epoch 0/200] [Batch 180/469] [D loss: 0.656812] [G loss: 0.545215]\n",
      "[Epoch 0/200] [Batch 181/469] [D loss: 0.601460] [G loss: 0.978298]\n",
      "[Epoch 0/200] [Batch 182/469] [D loss: 0.575999] [G loss: 0.720526]\n",
      "[Epoch 0/200] [Batch 183/469] [D loss: 0.560479] [G loss: 0.866425]\n",
      "[Epoch 0/200] [Batch 184/469] [D loss: 0.569406] [G loss: 0.853658]\n",
      "[Epoch 0/200] [Batch 185/469] [D loss: 0.562638] [G loss: 0.804308]\n",
      "[Epoch 0/200] [Batch 186/469] [D loss: 0.584273] [G loss: 0.929575]\n",
      "[Epoch 0/200] [Batch 187/469] [D loss: 0.600648] [G loss: 0.658128]\n",
      "[Epoch 0/200] [Batch 188/469] [D loss: 0.599299] [G loss: 1.042023]\n",
      "[Epoch 0/200] [Batch 189/469] [D loss: 0.626029] [G loss: 0.575877]\n",
      "[Epoch 0/200] [Batch 190/469] [D loss: 0.608734] [G loss: 1.175255]\n",
      "[Epoch 0/200] [Batch 191/469] [D loss: 0.699887] [G loss: 0.431437]\n",
      "[Epoch 0/200] [Batch 192/469] [D loss: 0.626197] [G loss: 1.261177]\n",
      "[Epoch 0/200] [Batch 193/469] [D loss: 0.635297] [G loss: 0.559940]\n",
      "[Epoch 0/200] [Batch 194/469] [D loss: 0.549291] [G loss: 1.043208]\n",
      "[Epoch 0/200] [Batch 195/469] [D loss: 0.543996] [G loss: 0.847073]\n",
      "[Epoch 0/200] [Batch 196/469] [D loss: 0.541736] [G loss: 0.804284]\n",
      "[Epoch 0/200] [Batch 197/469] [D loss: 0.517374] [G loss: 0.923220]\n",
      "[Epoch 0/200] [Batch 198/469] [D loss: 0.542841] [G loss: 0.906038]\n",
      "[Epoch 0/200] [Batch 199/469] [D loss: 0.554612] [G loss: 0.814634]\n",
      "[Epoch 0/200] [Batch 200/469] [D loss: 0.533979] [G loss: 0.949058]\n",
      "[Epoch 0/200] [Batch 201/469] [D loss: 0.526980] [G loss: 0.864440]\n",
      "[Epoch 0/200] [Batch 202/469] [D loss: 0.503062] [G loss: 0.909452]\n",
      "[Epoch 0/200] [Batch 203/469] [D loss: 0.517570] [G loss: 0.864253]\n",
      "[Epoch 0/200] [Batch 204/469] [D loss: 0.526771] [G loss: 0.892647]\n",
      "[Epoch 0/200] [Batch 205/469] [D loss: 0.567165] [G loss: 0.825814]\n",
      "[Epoch 0/200] [Batch 206/469] [D loss: 0.557455] [G loss: 0.831852]\n",
      "[Epoch 0/200] [Batch 207/469] [D loss: 0.586891] [G loss: 0.842541]\n",
      "[Epoch 0/200] [Batch 208/469] [D loss: 0.605703] [G loss: 0.716063]\n",
      "[Epoch 0/200] [Batch 209/469] [D loss: 0.594291] [G loss: 0.803291]\n",
      "[Epoch 0/200] [Batch 210/469] [D loss: 0.598194] [G loss: 0.715723]\n",
      "[Epoch 0/200] [Batch 211/469] [D loss: 0.555916] [G loss: 0.860346]\n",
      "[Epoch 0/200] [Batch 212/469] [D loss: 0.575140] [G loss: 0.883564]\n",
      "[Epoch 0/200] [Batch 213/469] [D loss: 0.537278] [G loss: 0.750084]\n",
      "[Epoch 0/200] [Batch 214/469] [D loss: 0.502904] [G loss: 1.202295]\n",
      "[Epoch 0/200] [Batch 215/469] [D loss: 0.539883] [G loss: 0.684716]\n",
      "[Epoch 0/200] [Batch 216/469] [D loss: 0.495899] [G loss: 1.300034]\n",
      "[Epoch 0/200] [Batch 217/469] [D loss: 0.518789] [G loss: 0.798177]\n",
      "[Epoch 0/200] [Batch 218/469] [D loss: 0.549191] [G loss: 1.057129]\n",
      "[Epoch 0/200] [Batch 219/469] [D loss: 0.555999] [G loss: 0.711316]\n",
      "[Epoch 0/200] [Batch 220/469] [D loss: 0.585321] [G loss: 1.192368]\n",
      "[Epoch 0/200] [Batch 221/469] [D loss: 0.636054] [G loss: 0.499548]\n",
      "[Epoch 0/200] [Batch 222/469] [D loss: 0.599118] [G loss: 1.172442]\n",
      "[Epoch 0/200] [Batch 223/469] [D loss: 0.612671] [G loss: 0.573281]\n",
      "[Epoch 0/200] [Batch 224/469] [D loss: 0.596416] [G loss: 1.034609]\n",
      "[Epoch 0/200] [Batch 225/469] [D loss: 0.573387] [G loss: 0.619840]\n",
      "[Epoch 0/200] [Batch 226/469] [D loss: 0.512524] [G loss: 1.034876]\n",
      "[Epoch 0/200] [Batch 227/469] [D loss: 0.511108] [G loss: 0.824445]\n",
      "[Epoch 0/200] [Batch 228/469] [D loss: 0.515331] [G loss: 0.859339]\n",
      "[Epoch 0/200] [Batch 229/469] [D loss: 0.479288] [G loss: 0.873571]\n",
      "[Epoch 0/200] [Batch 230/469] [D loss: 0.467763] [G loss: 1.012084]\n",
      "[Epoch 0/200] [Batch 231/469] [D loss: 0.496485] [G loss: 0.785215]\n",
      "[Epoch 0/200] [Batch 232/469] [D loss: 0.532790] [G loss: 0.974592]\n",
      "[Epoch 0/200] [Batch 233/469] [D loss: 0.587297] [G loss: 0.584928]\n",
      "[Epoch 0/200] [Batch 234/469] [D loss: 0.613309] [G loss: 1.127219]\n",
      "[Epoch 0/200] [Batch 235/469] [D loss: 0.731594] [G loss: 0.379950]\n",
      "[Epoch 0/200] [Batch 236/469] [D loss: 0.674595] [G loss: 1.100534]\n",
      "[Epoch 0/200] [Batch 237/469] [D loss: 0.683687] [G loss: 0.487711]\n",
      "[Epoch 0/200] [Batch 238/469] [D loss: 0.602394] [G loss: 0.763792]\n",
      "[Epoch 0/200] [Batch 239/469] [D loss: 0.587769] [G loss: 0.775655]\n",
      "[Epoch 0/200] [Batch 240/469] [D loss: 0.591242] [G loss: 0.707973]\n",
      "[Epoch 0/200] [Batch 241/469] [D loss: 0.586956] [G loss: 0.663599]\n",
      "[Epoch 0/200] [Batch 242/469] [D loss: 0.564350] [G loss: 0.738490]\n",
      "[Epoch 0/200] [Batch 243/469] [D loss: 0.559997] [G loss: 0.743454]\n",
      "[Epoch 0/200] [Batch 244/469] [D loss: 0.548603] [G loss: 0.659604]\n",
      "[Epoch 0/200] [Batch 245/469] [D loss: 0.524992] [G loss: 0.875344]\n",
      "[Epoch 0/200] [Batch 246/469] [D loss: 0.528403] [G loss: 0.687999]\n",
      "[Epoch 0/200] [Batch 247/469] [D loss: 0.522949] [G loss: 0.810631]\n",
      "[Epoch 0/200] [Batch 248/469] [D loss: 0.538446] [G loss: 0.726043]\n",
      "[Epoch 0/200] [Batch 249/469] [D loss: 0.513243] [G loss: 0.729267]\n",
      "[Epoch 0/200] [Batch 250/469] [D loss: 0.534449] [G loss: 0.913274]\n",
      "[Epoch 0/200] [Batch 251/469] [D loss: 0.548274] [G loss: 0.565231]\n",
      "[Epoch 0/200] [Batch 252/469] [D loss: 0.521766] [G loss: 1.034265]\n",
      "[Epoch 0/200] [Batch 253/469] [D loss: 0.536018] [G loss: 0.588096]\n",
      "[Epoch 0/200] [Batch 254/469] [D loss: 0.507339] [G loss: 0.930593]\n",
      "[Epoch 0/200] [Batch 255/469] [D loss: 0.509673] [G loss: 0.680076]\n",
      "[Epoch 0/200] [Batch 256/469] [D loss: 0.506451] [G loss: 0.834462]\n",
      "[Epoch 0/200] [Batch 257/469] [D loss: 0.527123] [G loss: 0.728648]\n",
      "[Epoch 0/200] [Batch 258/469] [D loss: 0.527572] [G loss: 0.738032]\n",
      "[Epoch 0/200] [Batch 259/469] [D loss: 0.531980] [G loss: 0.768790]\n",
      "[Epoch 0/200] [Batch 260/469] [D loss: 0.534405] [G loss: 0.683470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 261/469] [D loss: 0.542313] [G loss: 0.822623]\n",
      "[Epoch 0/200] [Batch 262/469] [D loss: 0.551716] [G loss: 0.602413]\n",
      "[Epoch 0/200] [Batch 263/469] [D loss: 0.565246] [G loss: 0.957908]\n",
      "[Epoch 0/200] [Batch 264/469] [D loss: 0.601810] [G loss: 0.499061]\n",
      "[Epoch 0/200] [Batch 265/469] [D loss: 0.603712] [G loss: 1.004744]\n",
      "[Epoch 0/200] [Batch 266/469] [D loss: 0.635820] [G loss: 0.451632]\n",
      "[Epoch 0/200] [Batch 267/469] [D loss: 0.581545] [G loss: 1.003213]\n",
      "[Epoch 0/200] [Batch 268/469] [D loss: 0.562607] [G loss: 0.559312]\n",
      "[Epoch 0/200] [Batch 269/469] [D loss: 0.527508] [G loss: 1.001637]\n",
      "[Epoch 0/200] [Batch 270/469] [D loss: 0.556225] [G loss: 0.635649]\n",
      "[Epoch 0/200] [Batch 271/469] [D loss: 0.564077] [G loss: 0.877012]\n",
      "[Epoch 0/200] [Batch 272/469] [D loss: 0.557413] [G loss: 0.655387]\n",
      "[Epoch 0/200] [Batch 273/469] [D loss: 0.562284] [G loss: 0.928673]\n",
      "[Epoch 0/200] [Batch 274/469] [D loss: 0.582246] [G loss: 0.607902]\n",
      "[Epoch 0/200] [Batch 275/469] [D loss: 0.574157] [G loss: 1.074048]\n",
      "[Epoch 0/200] [Batch 276/469] [D loss: 0.653586] [G loss: 0.493342]\n",
      "[Epoch 0/200] [Batch 277/469] [D loss: 0.624553] [G loss: 1.092420]\n",
      "[Epoch 0/200] [Batch 278/469] [D loss: 0.657242] [G loss: 0.456682]\n",
      "[Epoch 0/200] [Batch 279/469] [D loss: 0.654777] [G loss: 1.169522]\n",
      "[Epoch 0/200] [Batch 280/469] [D loss: 0.681130] [G loss: 0.421553]\n",
      "[Epoch 0/200] [Batch 281/469] [D loss: 0.590621] [G loss: 1.028911]\n",
      "[Epoch 0/200] [Batch 282/469] [D loss: 0.556859] [G loss: 0.720455]\n",
      "[Epoch 0/200] [Batch 283/469] [D loss: 0.552926] [G loss: 0.819479]\n",
      "[Epoch 0/200] [Batch 284/469] [D loss: 0.540030] [G loss: 0.863086]\n",
      "[Epoch 0/200] [Batch 285/469] [D loss: 0.545831] [G loss: 0.747094]\n",
      "[Epoch 0/200] [Batch 286/469] [D loss: 0.556653] [G loss: 0.898222]\n",
      "[Epoch 0/200] [Batch 287/469] [D loss: 0.576567] [G loss: 0.672199]\n",
      "[Epoch 0/200] [Batch 288/469] [D loss: 0.529189] [G loss: 0.901046]\n",
      "[Epoch 0/200] [Batch 289/469] [D loss: 0.540355] [G loss: 0.764937]\n",
      "[Epoch 0/200] [Batch 290/469] [D loss: 0.562541] [G loss: 0.842794]\n",
      "[Epoch 0/200] [Batch 291/469] [D loss: 0.567786] [G loss: 0.713406]\n",
      "[Epoch 0/200] [Batch 292/469] [D loss: 0.572442] [G loss: 0.953866]\n",
      "[Epoch 0/200] [Batch 293/469] [D loss: 0.600565] [G loss: 0.578095]\n",
      "[Epoch 0/200] [Batch 294/469] [D loss: 0.552789] [G loss: 1.280486]\n",
      "[Epoch 0/200] [Batch 295/469] [D loss: 0.588615] [G loss: 0.514654]\n",
      "[Epoch 0/200] [Batch 296/469] [D loss: 0.547579] [G loss: 1.351514]\n",
      "[Epoch 0/200] [Batch 297/469] [D loss: 0.545683] [G loss: 0.605763]\n",
      "[Epoch 0/200] [Batch 298/469] [D loss: 0.485824] [G loss: 1.201185]\n",
      "[Epoch 0/200] [Batch 299/469] [D loss: 0.479536] [G loss: 0.769542]\n",
      "[Epoch 0/200] [Batch 300/469] [D loss: 0.495680] [G loss: 1.065080]\n",
      "[Epoch 0/200] [Batch 301/469] [D loss: 0.496152] [G loss: 0.691758]\n",
      "[Epoch 0/200] [Batch 302/469] [D loss: 0.496215] [G loss: 1.309353]\n",
      "[Epoch 0/200] [Batch 303/469] [D loss: 0.534124] [G loss: 0.575430]\n",
      "[Epoch 0/200] [Batch 304/469] [D loss: 0.531994] [G loss: 1.377794]\n",
      "[Epoch 0/200] [Batch 305/469] [D loss: 0.600001] [G loss: 0.488737]\n",
      "[Epoch 0/200] [Batch 306/469] [D loss: 0.569612] [G loss: 1.379013]\n",
      "[Epoch 0/200] [Batch 307/469] [D loss: 0.608767] [G loss: 0.499041]\n",
      "[Epoch 0/200] [Batch 308/469] [D loss: 0.527239] [G loss: 1.300681]\n",
      "[Epoch 0/200] [Batch 309/469] [D loss: 0.555812] [G loss: 0.612791]\n",
      "[Epoch 0/200] [Batch 310/469] [D loss: 0.520237] [G loss: 1.227953]\n",
      "[Epoch 0/200] [Batch 311/469] [D loss: 0.563199] [G loss: 0.592231]\n",
      "[Epoch 0/200] [Batch 312/469] [D loss: 0.554077] [G loss: 1.222930]\n",
      "[Epoch 0/200] [Batch 313/469] [D loss: 0.620064] [G loss: 0.472619]\n",
      "[Epoch 0/200] [Batch 314/469] [D loss: 0.574904] [G loss: 1.476670]\n",
      "[Epoch 0/200] [Batch 315/469] [D loss: 0.620097] [G loss: 0.480722]\n",
      "[Epoch 0/200] [Batch 316/469] [D loss: 0.522737] [G loss: 1.229383]\n",
      "[Epoch 0/200] [Batch 317/469] [D loss: 0.517277] [G loss: 0.694891]\n",
      "[Epoch 0/200] [Batch 318/469] [D loss: 0.480350] [G loss: 1.124937]\n",
      "[Epoch 0/200] [Batch 319/469] [D loss: 0.499322] [G loss: 0.778866]\n",
      "[Epoch 0/200] [Batch 320/469] [D loss: 0.483430] [G loss: 1.091371]\n",
      "[Epoch 0/200] [Batch 321/469] [D loss: 0.511999] [G loss: 0.708202]\n",
      "[Epoch 0/200] [Batch 322/469] [D loss: 0.540225] [G loss: 1.278774]\n",
      "[Epoch 0/200] [Batch 323/469] [D loss: 0.674240] [G loss: 0.371686]\n",
      "[Epoch 0/200] [Batch 324/469] [D loss: 0.692193] [G loss: 1.937028]\n",
      "[Epoch 0/200] [Batch 325/469] [D loss: 0.751140] [G loss: 0.309034]\n",
      "[Epoch 0/200] [Batch 326/469] [D loss: 0.502277] [G loss: 1.155373]\n",
      "[Epoch 0/200] [Batch 327/469] [D loss: 0.506542] [G loss: 0.959814]\n",
      "[Epoch 0/200] [Batch 328/469] [D loss: 0.556486] [G loss: 0.614450]\n",
      "[Epoch 0/200] [Batch 329/469] [D loss: 0.545254] [G loss: 1.030075]\n",
      "[Epoch 0/200] [Batch 330/469] [D loss: 0.543308] [G loss: 0.701399]\n",
      "[Epoch 0/200] [Batch 331/469] [D loss: 0.534790] [G loss: 0.942803]\n",
      "[Epoch 0/200] [Batch 332/469] [D loss: 0.524240] [G loss: 0.788307]\n",
      "[Epoch 0/200] [Batch 333/469] [D loss: 0.508618] [G loss: 0.958678]\n",
      "[Epoch 0/200] [Batch 334/469] [D loss: 0.502106] [G loss: 0.832023]\n",
      "[Epoch 0/200] [Batch 335/469] [D loss: 0.456101] [G loss: 0.981720]\n",
      "[Epoch 0/200] [Batch 336/469] [D loss: 0.452531] [G loss: 0.955044]\n",
      "[Epoch 0/200] [Batch 337/469] [D loss: 0.486391] [G loss: 0.927499]\n",
      "[Epoch 0/200] [Batch 338/469] [D loss: 0.491642] [G loss: 0.821259]\n",
      "[Epoch 0/200] [Batch 339/469] [D loss: 0.490824] [G loss: 1.004032]\n",
      "[Epoch 0/200] [Batch 340/469] [D loss: 0.534331] [G loss: 0.651331]\n",
      "[Epoch 0/200] [Batch 341/469] [D loss: 0.542455] [G loss: 1.272363]\n",
      "[Epoch 0/200] [Batch 342/469] [D loss: 0.658024] [G loss: 0.382177]\n",
      "[Epoch 0/200] [Batch 343/469] [D loss: 0.618418] [G loss: 1.604271]\n",
      "[Epoch 0/200] [Batch 344/469] [D loss: 0.643859] [G loss: 0.384538]\n",
      "[Epoch 0/200] [Batch 345/469] [D loss: 0.479124] [G loss: 1.182204]\n",
      "[Epoch 0/200] [Batch 346/469] [D loss: 0.472082] [G loss: 0.886813]\n",
      "[Epoch 0/200] [Batch 347/469] [D loss: 0.500143] [G loss: 0.737938]\n",
      "[Epoch 0/200] [Batch 348/469] [D loss: 0.473843] [G loss: 0.987122]\n",
      "[Epoch 0/200] [Batch 349/469] [D loss: 0.497038] [G loss: 0.829214]\n",
      "[Epoch 0/200] [Batch 350/469] [D loss: 0.493352] [G loss: 0.843499]\n",
      "[Epoch 0/200] [Batch 351/469] [D loss: 0.481006] [G loss: 0.866269]\n",
      "[Epoch 0/200] [Batch 352/469] [D loss: 0.465427] [G loss: 0.860650]\n",
      "[Epoch 0/200] [Batch 353/469] [D loss: 0.459680] [G loss: 0.958796]\n",
      "[Epoch 0/200] [Batch 354/469] [D loss: 0.452461] [G loss: 0.802671]\n",
      "[Epoch 0/200] [Batch 355/469] [D loss: 0.433922] [G loss: 1.128867]\n",
      "[Epoch 0/200] [Batch 356/469] [D loss: 0.490158] [G loss: 0.653345]\n",
      "[Epoch 0/200] [Batch 357/469] [D loss: 0.514573] [G loss: 1.256278]\n",
      "[Epoch 0/200] [Batch 358/469] [D loss: 0.667624] [G loss: 0.375868]\n",
      "[Epoch 0/200] [Batch 359/469] [D loss: 0.599546] [G loss: 1.470746]\n",
      "[Epoch 0/200] [Batch 360/469] [D loss: 0.705352] [G loss: 0.338936]\n",
      "[Epoch 0/200] [Batch 361/469] [D loss: 0.508143] [G loss: 1.067902]\n",
      "[Epoch 0/200] [Batch 362/469] [D loss: 0.484870] [G loss: 0.737575]\n",
      "[Epoch 0/200] [Batch 363/469] [D loss: 0.507766] [G loss: 0.796038]\n",
      "[Epoch 0/200] [Batch 364/469] [D loss: 0.508851] [G loss: 0.790705]\n",
      "[Epoch 0/200] [Batch 365/469] [D loss: 0.504044] [G loss: 0.717339]\n",
      "[Epoch 0/200] [Batch 366/469] [D loss: 0.477750] [G loss: 0.893312]\n",
      "[Epoch 0/200] [Batch 367/469] [D loss: 0.473969] [G loss: 0.743661]\n",
      "[Epoch 0/200] [Batch 368/469] [D loss: 0.448390] [G loss: 0.893014]\n",
      "[Epoch 0/200] [Batch 369/469] [D loss: 0.431459] [G loss: 0.841578]\n",
      "[Epoch 0/200] [Batch 370/469] [D loss: 0.436035] [G loss: 0.931222]\n",
      "[Epoch 0/200] [Batch 371/469] [D loss: 0.448196] [G loss: 0.800085]\n",
      "[Epoch 0/200] [Batch 372/469] [D loss: 0.467286] [G loss: 0.972785]\n",
      "[Epoch 0/200] [Batch 373/469] [D loss: 0.518233] [G loss: 0.623135]\n",
      "[Epoch 0/200] [Batch 374/469] [D loss: 0.520960] [G loss: 1.155269]\n",
      "[Epoch 0/200] [Batch 375/469] [D loss: 0.601527] [G loss: 0.466943]\n",
      "[Epoch 0/200] [Batch 376/469] [D loss: 0.539021] [G loss: 1.102273]\n",
      "[Epoch 0/200] [Batch 377/469] [D loss: 0.535818] [G loss: 0.582860]\n",
      "[Epoch 0/200] [Batch 378/469] [D loss: 0.479850] [G loss: 0.899255]\n",
      "[Epoch 0/200] [Batch 379/469] [D loss: 0.468546] [G loss: 0.874013]\n",
      "[Epoch 0/200] [Batch 380/469] [D loss: 0.485032] [G loss: 0.774436]\n",
      "[Epoch 0/200] [Batch 381/469] [D loss: 0.478211] [G loss: 0.900560]\n",
      "[Epoch 0/200] [Batch 382/469] [D loss: 0.470383] [G loss: 0.780934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 383/469] [D loss: 0.495261] [G loss: 0.976531]\n",
      "[Epoch 0/200] [Batch 384/469] [D loss: 0.534115] [G loss: 0.606192]\n",
      "[Epoch 0/200] [Batch 385/469] [D loss: 0.593867] [G loss: 1.317514]\n",
      "[Epoch 0/200] [Batch 386/469] [D loss: 0.789377] [G loss: 0.302476]\n",
      "[Epoch 0/200] [Batch 387/469] [D loss: 0.577560] [G loss: 1.104918]\n",
      "[Epoch 0/200] [Batch 388/469] [D loss: 0.576022] [G loss: 0.754804]\n",
      "[Epoch 0/200] [Batch 389/469] [D loss: 0.581772] [G loss: 0.690908]\n",
      "[Epoch 0/200] [Batch 390/469] [D loss: 0.596950] [G loss: 0.857434]\n",
      "[Epoch 0/200] [Batch 391/469] [D loss: 0.614691] [G loss: 0.661198]\n",
      "[Epoch 0/200] [Batch 392/469] [D loss: 0.604495] [G loss: 0.777232]\n",
      "[Epoch 0/200] [Batch 393/469] [D loss: 0.588466] [G loss: 0.736637]\n",
      "[Epoch 0/200] [Batch 394/469] [D loss: 0.567743] [G loss: 0.792482]\n",
      "[Epoch 0/200] [Batch 395/469] [D loss: 0.553639] [G loss: 0.730305]\n",
      "[Epoch 0/200] [Batch 396/469] [D loss: 0.524035] [G loss: 0.892844]\n",
      "[Epoch 0/200] [Batch 397/469] [D loss: 0.525790] [G loss: 0.803476]\n",
      "[Epoch 0/200] [Batch 398/469] [D loss: 0.504144] [G loss: 0.851233]\n",
      "[Epoch 0/200] [Batch 399/469] [D loss: 0.508121] [G loss: 0.920323]\n",
      "[Epoch 0/200] [Batch 400/469] [D loss: 0.488259] [G loss: 0.789634]\n",
      "[Epoch 0/200] [Batch 401/469] [D loss: 0.468105] [G loss: 1.147668]\n",
      "[Epoch 0/200] [Batch 402/469] [D loss: 0.486840] [G loss: 0.749314]\n",
      "[Epoch 0/200] [Batch 403/469] [D loss: 0.480796] [G loss: 1.284086]\n",
      "[Epoch 0/200] [Batch 404/469] [D loss: 0.532218] [G loss: 0.597846]\n",
      "[Epoch 0/200] [Batch 405/469] [D loss: 0.512010] [G loss: 1.469033]\n",
      "[Epoch 0/200] [Batch 406/469] [D loss: 0.572295] [G loss: 0.514571]\n",
      "[Epoch 0/200] [Batch 407/469] [D loss: 0.542691] [G loss: 1.432596]\n",
      "[Epoch 0/200] [Batch 408/469] [D loss: 0.595506] [G loss: 0.499002]\n",
      "[Epoch 0/200] [Batch 409/469] [D loss: 0.588779] [G loss: 1.293760]\n",
      "[Epoch 0/200] [Batch 410/469] [D loss: 0.629354] [G loss: 0.450571]\n",
      "[Epoch 0/200] [Batch 411/469] [D loss: 0.574054] [G loss: 1.224012]\n",
      "[Epoch 0/200] [Batch 412/469] [D loss: 0.569840] [G loss: 0.623984]\n",
      "[Epoch 0/200] [Batch 413/469] [D loss: 0.529781] [G loss: 1.016686]\n",
      "[Epoch 0/200] [Batch 414/469] [D loss: 0.560677] [G loss: 0.794528]\n",
      "[Epoch 0/200] [Batch 415/469] [D loss: 0.543922] [G loss: 0.772501]\n",
      "[Epoch 0/200] [Batch 416/469] [D loss: 0.546017] [G loss: 0.994356]\n",
      "[Epoch 0/200] [Batch 417/469] [D loss: 0.562890] [G loss: 0.674715]\n",
      "[Epoch 0/200] [Batch 418/469] [D loss: 0.514741] [G loss: 1.146408]\n",
      "[Epoch 0/200] [Batch 419/469] [D loss: 0.526695] [G loss: 0.687773]\n",
      "[Epoch 0/200] [Batch 420/469] [D loss: 0.539166] [G loss: 1.249002]\n",
      "[Epoch 0/200] [Batch 421/469] [D loss: 0.589663] [G loss: 0.482106]\n",
      "[Epoch 0/200] [Batch 422/469] [D loss: 0.566946] [G loss: 1.594693]\n",
      "[Epoch 0/200] [Batch 423/469] [D loss: 0.580867] [G loss: 0.477796]\n",
      "[Epoch 0/200] [Batch 424/469] [D loss: 0.471702] [G loss: 1.340040]\n",
      "[Epoch 0/200] [Batch 425/469] [D loss: 0.446642] [G loss: 0.841172]\n",
      "[Epoch 0/200] [Batch 426/469] [D loss: 0.466534] [G loss: 1.109469]\n",
      "[Epoch 0/200] [Batch 427/469] [D loss: 0.484979] [G loss: 0.802547]\n",
      "[Epoch 0/200] [Batch 428/469] [D loss: 0.521790] [G loss: 1.210791]\n",
      "[Epoch 0/200] [Batch 429/469] [D loss: 0.598337] [G loss: 0.490654]\n",
      "[Epoch 0/200] [Batch 430/469] [D loss: 0.699636] [G loss: 1.799385]\n",
      "[Epoch 0/200] [Batch 431/469] [D loss: 0.856541] [G loss: 0.228737]\n",
      "[Epoch 0/200] [Batch 432/469] [D loss: 0.564393] [G loss: 1.501746]\n",
      "[Epoch 0/200] [Batch 433/469] [D loss: 0.490984] [G loss: 0.739915]\n",
      "[Epoch 0/200] [Batch 434/469] [D loss: 0.466936] [G loss: 0.928419]\n",
      "[Epoch 0/200] [Batch 435/469] [D loss: 0.460984] [G loss: 1.098732]\n",
      "[Epoch 0/200] [Batch 436/469] [D loss: 0.482013] [G loss: 0.814509]\n",
      "[Epoch 0/200] [Batch 437/469] [D loss: 0.488903] [G loss: 1.199154]\n",
      "[Epoch 0/200] [Batch 438/469] [D loss: 0.536437] [G loss: 0.645171]\n",
      "[Epoch 0/200] [Batch 439/469] [D loss: 0.567663] [G loss: 1.381100]\n",
      "[Epoch 0/200] [Batch 440/469] [D loss: 0.651951] [G loss: 0.413905]\n",
      "[Epoch 0/200] [Batch 441/469] [D loss: 0.607034] [G loss: 1.539673]\n",
      "[Epoch 0/200] [Batch 442/469] [D loss: 0.585832] [G loss: 0.503469]\n",
      "[Epoch 0/200] [Batch 443/469] [D loss: 0.469575] [G loss: 1.308997]\n",
      "[Epoch 0/200] [Batch 444/469] [D loss: 0.413806] [G loss: 0.972259]\n",
      "[Epoch 0/200] [Batch 445/469] [D loss: 0.396845] [G loss: 1.064071]\n",
      "[Epoch 0/200] [Batch 446/469] [D loss: 0.381176] [G loss: 1.208069]\n",
      "[Epoch 0/200] [Batch 447/469] [D loss: 0.418509] [G loss: 1.038853]\n",
      "[Epoch 0/200] [Batch 448/469] [D loss: 0.421024] [G loss: 1.028023]\n",
      "[Epoch 0/200] [Batch 449/469] [D loss: 0.458018] [G loss: 1.035115]\n",
      "[Epoch 0/200] [Batch 450/469] [D loss: 0.473424] [G loss: 0.873919]\n",
      "[Epoch 0/200] [Batch 451/469] [D loss: 0.485524] [G loss: 1.088115]\n",
      "[Epoch 0/200] [Batch 452/469] [D loss: 0.517061] [G loss: 0.722844]\n",
      "[Epoch 0/200] [Batch 453/469] [D loss: 0.521243] [G loss: 1.267783]\n",
      "[Epoch 0/200] [Batch 454/469] [D loss: 0.561502] [G loss: 0.526778]\n",
      "[Epoch 0/200] [Batch 455/469] [D loss: 0.575046] [G loss: 1.601408]\n",
      "[Epoch 0/200] [Batch 456/469] [D loss: 0.608774] [G loss: 0.440090]\n",
      "[Epoch 0/200] [Batch 457/469] [D loss: 0.529449] [G loss: 1.502842]\n",
      "[Epoch 0/200] [Batch 458/469] [D loss: 0.515614] [G loss: 0.622844]\n",
      "[Epoch 0/200] [Batch 459/469] [D loss: 0.437064] [G loss: 1.199869]\n",
      "[Epoch 0/200] [Batch 460/469] [D loss: 0.444976] [G loss: 0.949859]\n",
      "[Epoch 0/200] [Batch 461/469] [D loss: 0.433653] [G loss: 0.957934]\n",
      "[Epoch 0/200] [Batch 462/469] [D loss: 0.430984] [G loss: 1.165000]\n",
      "[Epoch 0/200] [Batch 463/469] [D loss: 0.429521] [G loss: 0.848271]\n",
      "[Epoch 0/200] [Batch 464/469] [D loss: 0.388398] [G loss: 1.327448]\n",
      "[Epoch 0/200] [Batch 465/469] [D loss: 0.375527] [G loss: 0.925717]\n",
      "[Epoch 0/200] [Batch 466/469] [D loss: 0.386494] [G loss: 1.428813]\n",
      "[Epoch 0/200] [Batch 467/469] [D loss: 0.414101] [G loss: 0.801166]\n",
      "[Epoch 0/200] [Batch 468/469] [D loss: 0.400973] [G loss: 1.621971]\n",
      "[Epoch 1/200] [Batch 0/469] [D loss: 0.453081] [G loss: 0.647016]\n",
      "[Epoch 1/200] [Batch 1/469] [D loss: 0.454618] [G loss: 1.769843]\n",
      "[Epoch 1/200] [Batch 2/469] [D loss: 0.525563] [G loss: 0.508087]\n",
      "[Epoch 1/200] [Batch 3/469] [D loss: 0.472733] [G loss: 1.696121]\n",
      "[Epoch 1/200] [Batch 4/469] [D loss: 0.494674] [G loss: 0.592581]\n",
      "[Epoch 1/200] [Batch 5/469] [D loss: 0.426575] [G loss: 1.532285]\n",
      "[Epoch 1/200] [Batch 6/469] [D loss: 0.425475] [G loss: 0.771285]\n",
      "[Epoch 1/200] [Batch 7/469] [D loss: 0.401285] [G loss: 1.395954]\n",
      "[Epoch 1/200] [Batch 8/469] [D loss: 0.429125] [G loss: 0.855262]\n",
      "[Epoch 1/200] [Batch 9/469] [D loss: 0.421258] [G loss: 1.286458]\n",
      "[Epoch 1/200] [Batch 10/469] [D loss: 0.430939] [G loss: 0.799079]\n",
      "[Epoch 1/200] [Batch 11/469] [D loss: 0.422713] [G loss: 1.490902]\n",
      "[Epoch 1/200] [Batch 12/469] [D loss: 0.485265] [G loss: 0.645407]\n",
      "[Epoch 1/200] [Batch 13/469] [D loss: 0.502038] [G loss: 1.613253]\n",
      "[Epoch 1/200] [Batch 14/469] [D loss: 0.602857] [G loss: 0.447640]\n",
      "[Epoch 1/200] [Batch 15/469] [D loss: 0.516011] [G loss: 1.931888]\n",
      "[Epoch 1/200] [Batch 16/469] [D loss: 0.495167] [G loss: 0.604447]\n",
      "[Epoch 1/200] [Batch 17/469] [D loss: 0.364590] [G loss: 1.447397]\n",
      "[Epoch 1/200] [Batch 18/469] [D loss: 0.345793] [G loss: 1.175352]\n",
      "[Epoch 1/200] [Batch 19/469] [D loss: 0.368145] [G loss: 1.110531]\n",
      "[Epoch 1/200] [Batch 20/469] [D loss: 0.362621] [G loss: 1.268192]\n",
      "[Epoch 1/200] [Batch 21/469] [D loss: 0.362044] [G loss: 1.065325]\n",
      "[Epoch 1/200] [Batch 22/469] [D loss: 0.348684] [G loss: 1.441071]\n",
      "[Epoch 1/200] [Batch 23/469] [D loss: 0.393265] [G loss: 0.878533]\n",
      "[Epoch 1/200] [Batch 24/469] [D loss: 0.414522] [G loss: 1.699093]\n",
      "[Epoch 1/200] [Batch 25/469] [D loss: 0.525760] [G loss: 0.511190]\n",
      "[Epoch 1/200] [Batch 26/469] [D loss: 0.567829] [G loss: 2.511124]\n",
      "[Epoch 1/200] [Batch 27/469] [D loss: 0.634141] [G loss: 0.365121]\n",
      "[Epoch 1/200] [Batch 28/469] [D loss: 0.394295] [G loss: 1.854806]\n",
      "[Epoch 1/200] [Batch 29/469] [D loss: 0.324825] [G loss: 1.075999]\n",
      "[Epoch 1/200] [Batch 30/469] [D loss: 0.328025] [G loss: 1.114809]\n",
      "[Epoch 1/200] [Batch 31/469] [D loss: 0.336533] [G loss: 1.386058]\n",
      "[Epoch 1/200] [Batch 32/469] [D loss: 0.387564] [G loss: 0.913100]\n",
      "[Epoch 1/200] [Batch 33/469] [D loss: 0.374853] [G loss: 1.279878]\n",
      "[Epoch 1/200] [Batch 34/469] [D loss: 0.426821] [G loss: 0.915463]\n",
      "[Epoch 1/200] [Batch 35/469] [D loss: 0.418819] [G loss: 1.069671]\n",
      "[Epoch 1/200] [Batch 36/469] [D loss: 0.409496] [G loss: 0.948440]\n",
      "[Epoch 1/200] [Batch 37/469] [D loss: 0.461585] [G loss: 1.176543]\n",
      "[Epoch 1/200] [Batch 38/469] [D loss: 0.531605] [G loss: 0.535564]\n",
      "[Epoch 1/200] [Batch 39/469] [D loss: 0.548370] [G loss: 1.883861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 40/469] [D loss: 0.635230] [G loss: 0.381984]\n",
      "[Epoch 1/200] [Batch 41/469] [D loss: 0.367096] [G loss: 1.507630]\n",
      "[Epoch 1/200] [Batch 42/469] [D loss: 0.341622] [G loss: 1.158306]\n",
      "[Epoch 1/200] [Batch 43/469] [D loss: 0.366492] [G loss: 0.929082]\n",
      "[Epoch 1/200] [Batch 44/469] [D loss: 0.356781] [G loss: 1.262566]\n",
      "[Epoch 1/200] [Batch 45/469] [D loss: 0.361517] [G loss: 0.965257]\n",
      "[Epoch 1/200] [Batch 46/469] [D loss: 0.374594] [G loss: 1.121864]\n",
      "[Epoch 1/200] [Batch 47/469] [D loss: 0.388268] [G loss: 0.964411]\n",
      "[Epoch 1/200] [Batch 48/469] [D loss: 0.381801] [G loss: 1.077340]\n",
      "[Epoch 1/200] [Batch 49/469] [D loss: 0.405330] [G loss: 0.863467]\n",
      "[Epoch 1/200] [Batch 50/469] [D loss: 0.407263] [G loss: 1.186506]\n",
      "[Epoch 1/200] [Batch 51/469] [D loss: 0.427349] [G loss: 0.704260]\n",
      "[Epoch 1/200] [Batch 52/469] [D loss: 0.408363] [G loss: 1.431227]\n",
      "[Epoch 1/200] [Batch 53/469] [D loss: 0.440880] [G loss: 0.664591]\n",
      "[Epoch 1/200] [Batch 54/469] [D loss: 0.400384] [G loss: 1.419920]\n",
      "[Epoch 1/200] [Batch 55/469] [D loss: 0.418938] [G loss: 0.713714]\n",
      "[Epoch 1/200] [Batch 56/469] [D loss: 0.347110] [G loss: 1.394633]\n",
      "[Epoch 1/200] [Batch 57/469] [D loss: 0.358842] [G loss: 0.967339]\n",
      "[Epoch 1/200] [Batch 58/469] [D loss: 0.341570] [G loss: 1.015229]\n",
      "[Epoch 1/200] [Batch 59/469] [D loss: 0.345403] [G loss: 1.147123]\n",
      "[Epoch 1/200] [Batch 60/469] [D loss: 0.336353] [G loss: 0.957048]\n",
      "[Epoch 1/200] [Batch 61/469] [D loss: 0.354690] [G loss: 1.269666]\n",
      "[Epoch 1/200] [Batch 62/469] [D loss: 0.389782] [G loss: 0.759735]\n",
      "[Epoch 1/200] [Batch 63/469] [D loss: 0.430604] [G loss: 1.726002]\n",
      "[Epoch 1/200] [Batch 64/469] [D loss: 0.527635] [G loss: 0.511562]\n",
      "[Epoch 1/200] [Batch 65/469] [D loss: 0.431014] [G loss: 2.006141]\n",
      "[Epoch 1/200] [Batch 66/469] [D loss: 0.405742] [G loss: 0.670055]\n",
      "[Epoch 1/200] [Batch 67/469] [D loss: 0.312729] [G loss: 1.378999]\n",
      "[Epoch 1/200] [Batch 68/469] [D loss: 0.344256] [G loss: 1.186557]\n",
      "[Epoch 1/200] [Batch 69/469] [D loss: 0.397329] [G loss: 0.866476]\n",
      "[Epoch 1/200] [Batch 70/469] [D loss: 0.486004] [G loss: 1.501161]\n",
      "[Epoch 1/200] [Batch 71/469] [D loss: 0.699107] [G loss: 0.359516]\n",
      "[Epoch 1/200] [Batch 72/469] [D loss: 0.657799] [G loss: 1.941700]\n",
      "[Epoch 1/200] [Batch 73/469] [D loss: 0.750751] [G loss: 0.299694]\n",
      "[Epoch 1/200] [Batch 74/469] [D loss: 0.376385] [G loss: 1.271644]\n",
      "[Epoch 1/200] [Batch 75/469] [D loss: 0.404929] [G loss: 1.580684]\n",
      "[Epoch 1/200] [Batch 76/469] [D loss: 0.486550] [G loss: 0.623424]\n",
      "[Epoch 1/200] [Batch 77/469] [D loss: 0.396957] [G loss: 1.493081]\n",
      "[Epoch 1/200] [Batch 78/469] [D loss: 0.425480] [G loss: 0.961822]\n",
      "[Epoch 1/200] [Batch 79/469] [D loss: 0.422131] [G loss: 0.986660]\n",
      "[Epoch 1/200] [Batch 80/469] [D loss: 0.439530] [G loss: 1.221475]\n",
      "[Epoch 1/200] [Batch 81/469] [D loss: 0.512189] [G loss: 0.672685]\n",
      "[Epoch 1/200] [Batch 82/469] [D loss: 0.513967] [G loss: 1.438003]\n",
      "[Epoch 1/200] [Batch 83/469] [D loss: 0.662584] [G loss: 0.394739]\n",
      "[Epoch 1/200] [Batch 84/469] [D loss: 0.689858] [G loss: 2.143357]\n",
      "[Epoch 1/200] [Batch 85/469] [D loss: 0.801920] [G loss: 0.284429]\n",
      "[Epoch 1/200] [Batch 86/469] [D loss: 0.424807] [G loss: 1.344827]\n",
      "[Epoch 1/200] [Batch 87/469] [D loss: 0.437792] [G loss: 1.657101]\n",
      "[Epoch 1/200] [Batch 88/469] [D loss: 0.484852] [G loss: 0.699915]\n",
      "[Epoch 1/200] [Batch 89/469] [D loss: 0.405457] [G loss: 1.363256]\n",
      "[Epoch 1/200] [Batch 90/469] [D loss: 0.428775] [G loss: 1.126590]\n",
      "[Epoch 1/200] [Batch 91/469] [D loss: 0.481427] [G loss: 0.847499]\n",
      "[Epoch 1/200] [Batch 92/469] [D loss: 0.473394] [G loss: 1.165048]\n",
      "[Epoch 1/200] [Batch 93/469] [D loss: 0.469885] [G loss: 0.835196]\n",
      "[Epoch 1/200] [Batch 94/469] [D loss: 0.495937] [G loss: 1.372358]\n",
      "[Epoch 1/200] [Batch 95/469] [D loss: 0.540571] [G loss: 0.636815]\n",
      "[Epoch 1/200] [Batch 96/469] [D loss: 0.517481] [G loss: 1.614567]\n",
      "[Epoch 1/200] [Batch 97/469] [D loss: 0.518413] [G loss: 0.614476]\n",
      "[Epoch 1/200] [Batch 98/469] [D loss: 0.432000] [G loss: 1.625472]\n",
      "[Epoch 1/200] [Batch 99/469] [D loss: 0.408083] [G loss: 0.908292]\n",
      "[Epoch 1/200] [Batch 100/469] [D loss: 0.411359] [G loss: 1.316571]\n",
      "[Epoch 1/200] [Batch 101/469] [D loss: 0.399647] [G loss: 0.882898]\n",
      "[Epoch 1/200] [Batch 102/469] [D loss: 0.377251] [G loss: 1.565071]\n",
      "[Epoch 1/200] [Batch 103/469] [D loss: 0.422634] [G loss: 0.820275]\n",
      "[Epoch 1/200] [Batch 104/469] [D loss: 0.441382] [G loss: 1.477233]\n",
      "[Epoch 1/200] [Batch 105/469] [D loss: 0.513779] [G loss: 0.558468]\n",
      "[Epoch 1/200] [Batch 106/469] [D loss: 0.554011] [G loss: 2.103135]\n",
      "[Epoch 1/200] [Batch 107/469] [D loss: 0.690111] [G loss: 0.337006]\n",
      "[Epoch 1/200] [Batch 108/469] [D loss: 0.442965] [G loss: 1.686612]\n",
      "[Epoch 1/200] [Batch 109/469] [D loss: 0.415655] [G loss: 0.827384]\n",
      "[Epoch 1/200] [Batch 110/469] [D loss: 0.388367] [G loss: 1.117989]\n",
      "[Epoch 1/200] [Batch 111/469] [D loss: 0.411079] [G loss: 1.105254]\n",
      "[Epoch 1/200] [Batch 112/469] [D loss: 0.448894] [G loss: 0.867647]\n",
      "[Epoch 1/200] [Batch 113/469] [D loss: 0.401141] [G loss: 1.114354]\n",
      "[Epoch 1/200] [Batch 114/469] [D loss: 0.458733] [G loss: 0.997851]\n",
      "[Epoch 1/200] [Batch 115/469] [D loss: 0.450722] [G loss: 0.764792]\n",
      "[Epoch 1/200] [Batch 116/469] [D loss: 0.552827] [G loss: 1.626244]\n",
      "[Epoch 1/200] [Batch 117/469] [D loss: 0.757324] [G loss: 0.321457]\n",
      "[Epoch 1/200] [Batch 118/469] [D loss: 0.618238] [G loss: 2.044721]\n",
      "[Epoch 1/200] [Batch 119/469] [D loss: 0.580661] [G loss: 0.532937]\n",
      "[Epoch 1/200] [Batch 120/469] [D loss: 0.389745] [G loss: 1.278675]\n",
      "[Epoch 1/200] [Batch 121/469] [D loss: 0.418633] [G loss: 1.325672]\n",
      "[Epoch 1/200] [Batch 122/469] [D loss: 0.465142] [G loss: 0.695365]\n",
      "[Epoch 1/200] [Batch 123/469] [D loss: 0.425511] [G loss: 1.522194]\n",
      "[Epoch 1/200] [Batch 124/469] [D loss: 0.439116] [G loss: 0.796171]\n",
      "[Epoch 1/200] [Batch 125/469] [D loss: 0.422559] [G loss: 1.227545]\n",
      "[Epoch 1/200] [Batch 126/469] [D loss: 0.416846] [G loss: 0.925318]\n",
      "[Epoch 1/200] [Batch 127/469] [D loss: 0.388705] [G loss: 1.252800]\n",
      "[Epoch 1/200] [Batch 128/469] [D loss: 0.393629] [G loss: 0.964054]\n",
      "[Epoch 1/200] [Batch 129/469] [D loss: 0.386127] [G loss: 1.195734]\n",
      "[Epoch 1/200] [Batch 130/469] [D loss: 0.421839] [G loss: 0.996798]\n",
      "[Epoch 1/200] [Batch 131/469] [D loss: 0.416753] [G loss: 1.004136]\n",
      "[Epoch 1/200] [Batch 132/469] [D loss: 0.421942] [G loss: 1.097786]\n",
      "[Epoch 1/200] [Batch 133/469] [D loss: 0.432636] [G loss: 0.924632]\n",
      "[Epoch 1/200] [Batch 134/469] [D loss: 0.409939] [G loss: 1.103596]\n",
      "[Epoch 1/200] [Batch 135/469] [D loss: 0.380717] [G loss: 1.035000]\n",
      "[Epoch 1/200] [Batch 136/469] [D loss: 0.435401] [G loss: 1.265710]\n",
      "[Epoch 1/200] [Batch 137/469] [D loss: 0.500066] [G loss: 0.609502]\n",
      "[Epoch 1/200] [Batch 138/469] [D loss: 0.590020] [G loss: 2.107997]\n",
      "[Epoch 1/200] [Batch 139/469] [D loss: 0.806532] [G loss: 0.252160]\n",
      "[Epoch 1/200] [Batch 140/469] [D loss: 0.341284] [G loss: 1.598823]\n",
      "[Epoch 1/200] [Batch 141/469] [D loss: 0.358933] [G loss: 1.623489]\n",
      "[Epoch 1/200] [Batch 142/469] [D loss: 0.418856] [G loss: 0.714156]\n",
      "[Epoch 1/200] [Batch 143/469] [D loss: 0.336027] [G loss: 1.441111]\n",
      "[Epoch 1/200] [Batch 144/469] [D loss: 0.325103] [G loss: 1.237230]\n",
      "[Epoch 1/200] [Batch 145/469] [D loss: 0.361419] [G loss: 1.015091]\n",
      "[Epoch 1/200] [Batch 146/469] [D loss: 0.371225] [G loss: 1.219499]\n",
      "[Epoch 1/200] [Batch 147/469] [D loss: 0.417519] [G loss: 0.899187]\n",
      "[Epoch 1/200] [Batch 148/469] [D loss: 0.424319] [G loss: 1.134636]\n",
      "[Epoch 1/200] [Batch 149/469] [D loss: 0.445908] [G loss: 0.807922]\n",
      "[Epoch 1/200] [Batch 150/469] [D loss: 0.446610] [G loss: 1.412320]\n",
      "[Epoch 1/200] [Batch 151/469] [D loss: 0.534188] [G loss: 0.509732]\n",
      "[Epoch 1/200] [Batch 152/469] [D loss: 0.544630] [G loss: 2.044218]\n",
      "[Epoch 1/200] [Batch 153/469] [D loss: 0.740753] [G loss: 0.301025]\n",
      "[Epoch 1/200] [Batch 154/469] [D loss: 0.485005] [G loss: 1.722554]\n",
      "[Epoch 1/200] [Batch 155/469] [D loss: 0.477920] [G loss: 0.663499]\n",
      "[Epoch 1/200] [Batch 156/469] [D loss: 0.435838] [G loss: 1.230768]\n",
      "[Epoch 1/200] [Batch 157/469] [D loss: 0.456270] [G loss: 0.826427]\n",
      "[Epoch 1/200] [Batch 158/469] [D loss: 0.432364] [G loss: 1.079885]\n",
      "[Epoch 1/200] [Batch 159/469] [D loss: 0.469907] [G loss: 0.930972]\n",
      "[Epoch 1/200] [Batch 160/469] [D loss: 0.494237] [G loss: 0.837273]\n",
      "[Epoch 1/200] [Batch 161/469] [D loss: 0.436564] [G loss: 1.056715]\n",
      "[Epoch 1/200] [Batch 162/469] [D loss: 0.436146] [G loss: 0.860571]\n",
      "[Epoch 1/200] [Batch 163/469] [D loss: 0.446790] [G loss: 1.217713]\n",
      "[Epoch 1/200] [Batch 164/469] [D loss: 0.464578] [G loss: 0.672965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 165/469] [D loss: 0.470695] [G loss: 1.769358]\n",
      "[Epoch 1/200] [Batch 166/469] [D loss: 0.606013] [G loss: 0.456546]\n",
      "[Epoch 1/200] [Batch 167/469] [D loss: 0.460651] [G loss: 1.777956]\n",
      "[Epoch 1/200] [Batch 168/469] [D loss: 0.450969] [G loss: 0.733152]\n",
      "[Epoch 1/200] [Batch 169/469] [D loss: 0.346074] [G loss: 1.386763]\n",
      "[Epoch 1/200] [Batch 170/469] [D loss: 0.381660] [G loss: 1.146583]\n",
      "[Epoch 1/200] [Batch 171/469] [D loss: 0.439842] [G loss: 0.863319]\n",
      "[Epoch 1/200] [Batch 172/469] [D loss: 0.425606] [G loss: 1.343529]\n",
      "[Epoch 1/200] [Batch 173/469] [D loss: 0.496758] [G loss: 0.700015]\n",
      "[Epoch 1/200] [Batch 174/469] [D loss: 0.490177] [G loss: 1.613183]\n",
      "[Epoch 1/200] [Batch 175/469] [D loss: 0.600258] [G loss: 0.483243]\n",
      "[Epoch 1/200] [Batch 176/469] [D loss: 0.462394] [G loss: 1.636812]\n",
      "[Epoch 1/200] [Batch 177/469] [D loss: 0.466867] [G loss: 0.761545]\n",
      "[Epoch 1/200] [Batch 178/469] [D loss: 0.392553] [G loss: 1.387416]\n",
      "[Epoch 1/200] [Batch 179/469] [D loss: 0.378076] [G loss: 1.024965]\n",
      "[Epoch 1/200] [Batch 180/469] [D loss: 0.369452] [G loss: 1.312132]\n",
      "[Epoch 1/200] [Batch 181/469] [D loss: 0.356555] [G loss: 1.053977]\n",
      "[Epoch 1/200] [Batch 182/469] [D loss: 0.357298] [G loss: 1.416850]\n",
      "[Epoch 1/200] [Batch 183/469] [D loss: 0.390792] [G loss: 0.972482]\n",
      "[Epoch 1/200] [Batch 184/469] [D loss: 0.386543] [G loss: 1.466653]\n",
      "[Epoch 1/200] [Batch 185/469] [D loss: 0.493028] [G loss: 0.655232]\n",
      "[Epoch 1/200] [Batch 186/469] [D loss: 0.551848] [G loss: 1.971234]\n",
      "[Epoch 1/200] [Batch 187/469] [D loss: 0.748830] [G loss: 0.422474]\n",
      "[Epoch 1/200] [Batch 188/469] [D loss: 0.441158] [G loss: 1.522874]\n",
      "[Epoch 1/200] [Batch 189/469] [D loss: 0.405574] [G loss: 1.244889]\n",
      "[Epoch 1/200] [Batch 190/469] [D loss: 0.453260] [G loss: 0.757844]\n",
      "[Epoch 1/200] [Batch 191/469] [D loss: 0.463220] [G loss: 1.407613]\n",
      "[Epoch 1/200] [Batch 192/469] [D loss: 0.532796] [G loss: 0.601735]\n",
      "[Epoch 1/200] [Batch 193/469] [D loss: 0.448325] [G loss: 1.319405]\n",
      "[Epoch 1/200] [Batch 194/469] [D loss: 0.467657] [G loss: 0.797018]\n",
      "[Epoch 1/200] [Batch 195/469] [D loss: 0.436419] [G loss: 1.210836]\n",
      "[Epoch 1/200] [Batch 196/469] [D loss: 0.464599] [G loss: 0.879459]\n",
      "[Epoch 1/200] [Batch 197/469] [D loss: 0.445765] [G loss: 1.075441]\n",
      "[Epoch 1/200] [Batch 198/469] [D loss: 0.435886] [G loss: 0.944267]\n",
      "[Epoch 1/200] [Batch 199/469] [D loss: 0.454985] [G loss: 1.050939]\n",
      "[Epoch 1/200] [Batch 200/469] [D loss: 0.452036] [G loss: 0.879720]\n",
      "[Epoch 1/200] [Batch 201/469] [D loss: 0.446314] [G loss: 1.097340]\n",
      "[Epoch 1/200] [Batch 202/469] [D loss: 0.450747] [G loss: 0.914513]\n",
      "[Epoch 1/200] [Batch 203/469] [D loss: 0.431227] [G loss: 1.062715]\n",
      "[Epoch 1/200] [Batch 204/469] [D loss: 0.428983] [G loss: 0.885907]\n",
      "[Epoch 1/200] [Batch 205/469] [D loss: 0.463632] [G loss: 1.316486]\n",
      "[Epoch 1/200] [Batch 206/469] [D loss: 0.595726] [G loss: 0.491041]\n",
      "[Epoch 1/200] [Batch 207/469] [D loss: 0.622601] [G loss: 2.088530]\n",
      "[Epoch 1/200] [Batch 208/469] [D loss: 0.812029] [G loss: 0.320683]\n",
      "[Epoch 1/200] [Batch 209/469] [D loss: 0.384388] [G loss: 1.364766]\n",
      "[Epoch 1/200] [Batch 210/469] [D loss: 0.448124] [G loss: 1.590314]\n",
      "[Epoch 1/200] [Batch 211/469] [D loss: 0.607930] [G loss: 0.458866]\n",
      "[Epoch 1/200] [Batch 212/469] [D loss: 0.473993] [G loss: 1.473398]\n",
      "[Epoch 1/200] [Batch 213/469] [D loss: 0.445774] [G loss: 0.720473]\n",
      "[Epoch 1/200] [Batch 214/469] [D loss: 0.404063] [G loss: 1.290081]\n",
      "[Epoch 1/200] [Batch 215/469] [D loss: 0.451582] [G loss: 0.831561]\n",
      "[Epoch 1/200] [Batch 216/469] [D loss: 0.457320] [G loss: 1.204922]\n",
      "[Epoch 1/200] [Batch 217/469] [D loss: 0.518877] [G loss: 0.646254]\n",
      "[Epoch 1/200] [Batch 218/469] [D loss: 0.517654] [G loss: 1.567224]\n",
      "[Epoch 1/200] [Batch 219/469] [D loss: 0.624819] [G loss: 0.438839]\n",
      "[Epoch 1/200] [Batch 220/469] [D loss: 0.493162] [G loss: 1.558166]\n",
      "[Epoch 1/200] [Batch 221/469] [D loss: 0.510844] [G loss: 0.572792]\n",
      "[Epoch 1/200] [Batch 222/469] [D loss: 0.436307] [G loss: 1.502156]\n",
      "[Epoch 1/200] [Batch 223/469] [D loss: 0.455604] [G loss: 0.773879]\n",
      "[Epoch 1/200] [Batch 224/469] [D loss: 0.386049] [G loss: 1.079120]\n",
      "[Epoch 1/200] [Batch 225/469] [D loss: 0.416641] [G loss: 1.261836]\n",
      "[Epoch 1/200] [Batch 226/469] [D loss: 0.451555] [G loss: 0.730842]\n",
      "[Epoch 1/200] [Batch 227/469] [D loss: 0.467011] [G loss: 1.550585]\n",
      "[Epoch 1/200] [Batch 228/469] [D loss: 0.585769] [G loss: 0.482541]\n",
      "[Epoch 1/200] [Batch 229/469] [D loss: 0.520660] [G loss: 1.796634]\n",
      "[Epoch 1/200] [Batch 230/469] [D loss: 0.600389] [G loss: 0.439200]\n",
      "[Epoch 1/200] [Batch 231/469] [D loss: 0.552766] [G loss: 1.911359]\n",
      "[Epoch 1/200] [Batch 232/469] [D loss: 0.601670] [G loss: 0.453948]\n",
      "[Epoch 1/200] [Batch 233/469] [D loss: 0.444200] [G loss: 1.519040]\n",
      "[Epoch 1/200] [Batch 234/469] [D loss: 0.454755] [G loss: 0.911812]\n",
      "[Epoch 1/200] [Batch 235/469] [D loss: 0.432426] [G loss: 1.061698]\n",
      "[Epoch 1/200] [Batch 236/469] [D loss: 0.426681] [G loss: 1.133222]\n",
      "[Epoch 1/200] [Batch 237/469] [D loss: 0.441137] [G loss: 0.895793]\n",
      "[Epoch 1/200] [Batch 238/469] [D loss: 0.454761] [G loss: 1.170794]\n",
      "[Epoch 1/200] [Batch 239/469] [D loss: 0.504621] [G loss: 0.676633]\n",
      "[Epoch 1/200] [Batch 240/469] [D loss: 0.556805] [G loss: 1.754900]\n",
      "[Epoch 1/200] [Batch 241/469] [D loss: 0.855365] [G loss: 0.221234]\n",
      "[Epoch 1/200] [Batch 242/469] [D loss: 0.688203] [G loss: 2.481144]\n",
      "[Epoch 1/200] [Batch 243/469] [D loss: 0.636606] [G loss: 0.397966]\n",
      "[Epoch 1/200] [Batch 244/469] [D loss: 0.394576] [G loss: 1.576139]\n",
      "[Epoch 1/200] [Batch 245/469] [D loss: 0.385621] [G loss: 1.293583]\n",
      "[Epoch 1/200] [Batch 246/469] [D loss: 0.470815] [G loss: 0.745297]\n",
      "[Epoch 1/200] [Batch 247/469] [D loss: 0.468003] [G loss: 1.512486]\n",
      "[Epoch 1/200] [Batch 248/469] [D loss: 0.580823] [G loss: 0.488781]\n",
      "[Epoch 1/200] [Batch 249/469] [D loss: 0.633718] [G loss: 1.840212]\n",
      "[Epoch 1/200] [Batch 250/469] [D loss: 0.879137] [G loss: 0.225086]\n",
      "[Epoch 1/200] [Batch 251/469] [D loss: 0.643137] [G loss: 1.716975]\n",
      "[Epoch 1/200] [Batch 252/469] [D loss: 0.614383] [G loss: 0.488078]\n",
      "[Epoch 1/200] [Batch 253/469] [D loss: 0.520545] [G loss: 1.176774]\n",
      "[Epoch 1/200] [Batch 254/469] [D loss: 0.531249] [G loss: 0.903745]\n",
      "[Epoch 1/200] [Batch 255/469] [D loss: 0.538071] [G loss: 0.747431]\n",
      "[Epoch 1/200] [Batch 256/469] [D loss: 0.532086] [G loss: 1.190875]\n",
      "[Epoch 1/200] [Batch 257/469] [D loss: 0.585417] [G loss: 0.611611]\n",
      "[Epoch 1/200] [Batch 258/469] [D loss: 0.571482] [G loss: 1.169317]\n",
      "[Epoch 1/200] [Batch 259/469] [D loss: 0.608032] [G loss: 0.606081]\n",
      "[Epoch 1/200] [Batch 260/469] [D loss: 0.578022] [G loss: 1.094152]\n",
      "[Epoch 1/200] [Batch 261/469] [D loss: 0.582522] [G loss: 0.676453]\n",
      "[Epoch 1/200] [Batch 262/469] [D loss: 0.555802] [G loss: 0.977745]\n",
      "[Epoch 1/200] [Batch 263/469] [D loss: 0.532322] [G loss: 0.829991]\n",
      "[Epoch 1/200] [Batch 264/469] [D loss: 0.525260] [G loss: 0.902499]\n",
      "[Epoch 1/200] [Batch 265/469] [D loss: 0.502467] [G loss: 0.936587]\n",
      "[Epoch 1/200] [Batch 266/469] [D loss: 0.503300] [G loss: 0.919504]\n",
      "[Epoch 1/200] [Batch 267/469] [D loss: 0.493712] [G loss: 0.984669]\n",
      "[Epoch 1/200] [Batch 268/469] [D loss: 0.484852] [G loss: 0.952991]\n",
      "[Epoch 1/200] [Batch 269/469] [D loss: 0.480974] [G loss: 1.029421]\n",
      "[Epoch 1/200] [Batch 270/469] [D loss: 0.474517] [G loss: 0.961980]\n",
      "[Epoch 1/200] [Batch 271/469] [D loss: 0.457808] [G loss: 0.986905]\n",
      "[Epoch 1/200] [Batch 272/469] [D loss: 0.500949] [G loss: 1.073783]\n",
      "[Epoch 1/200] [Batch 273/469] [D loss: 0.526287] [G loss: 0.719527]\n",
      "[Epoch 1/200] [Batch 274/469] [D loss: 0.518909] [G loss: 1.358399]\n",
      "[Epoch 1/200] [Batch 275/469] [D loss: 0.589540] [G loss: 0.512648]\n",
      "[Epoch 1/200] [Batch 276/469] [D loss: 0.597329] [G loss: 1.820227]\n",
      "[Epoch 1/200] [Batch 277/469] [D loss: 0.689753] [G loss: 0.364325]\n",
      "[Epoch 1/200] [Batch 278/469] [D loss: 0.536916] [G loss: 1.630075]\n",
      "[Epoch 1/200] [Batch 279/469] [D loss: 0.574998] [G loss: 0.624067]\n",
      "[Epoch 1/200] [Batch 280/469] [D loss: 0.470691] [G loss: 1.054514]\n",
      "[Epoch 1/200] [Batch 281/469] [D loss: 0.495494] [G loss: 1.018549]\n",
      "[Epoch 1/200] [Batch 282/469] [D loss: 0.531007] [G loss: 0.743228]\n",
      "[Epoch 1/200] [Batch 283/469] [D loss: 0.545373] [G loss: 1.129169]\n",
      "[Epoch 1/200] [Batch 284/469] [D loss: 0.637167] [G loss: 0.586122]\n",
      "[Epoch 1/200] [Batch 285/469] [D loss: 0.561250] [G loss: 1.090401]\n",
      "[Epoch 1/200] [Batch 286/469] [D loss: 0.536900] [G loss: 0.706312]\n",
      "[Epoch 1/200] [Batch 287/469] [D loss: 0.511750] [G loss: 1.170244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 288/469] [D loss: 0.532779] [G loss: 0.635873]\n",
      "[Epoch 1/200] [Batch 289/469] [D loss: 0.536750] [G loss: 1.561669]\n",
      "[Epoch 1/200] [Batch 290/469] [D loss: 0.685221] [G loss: 0.383034]\n",
      "[Epoch 1/200] [Batch 291/469] [D loss: 0.590060] [G loss: 1.933318]\n",
      "[Epoch 1/200] [Batch 292/469] [D loss: 0.622755] [G loss: 0.445991]\n",
      "[Epoch 1/200] [Batch 293/469] [D loss: 0.522949] [G loss: 1.507712]\n",
      "[Epoch 1/200] [Batch 294/469] [D loss: 0.501058] [G loss: 0.704666]\n",
      "[Epoch 1/200] [Batch 295/469] [D loss: 0.469594] [G loss: 1.149466]\n",
      "[Epoch 1/200] [Batch 296/469] [D loss: 0.459767] [G loss: 0.902356]\n",
      "[Epoch 1/200] [Batch 297/469] [D loss: 0.460158] [G loss: 1.047071]\n",
      "[Epoch 1/200] [Batch 298/469] [D loss: 0.467828] [G loss: 0.930004]\n",
      "[Epoch 1/200] [Batch 299/469] [D loss: 0.441110] [G loss: 1.012365]\n",
      "[Epoch 1/200] [Batch 300/469] [D loss: 0.453304] [G loss: 1.010952]\n",
      "[Epoch 1/200] [Batch 301/469] [D loss: 0.458615] [G loss: 0.902117]\n",
      "[Epoch 1/200] [Batch 302/469] [D loss: 0.459042] [G loss: 1.037092]\n",
      "[Epoch 1/200] [Batch 303/469] [D loss: 0.477027] [G loss: 0.828344]\n",
      "[Epoch 1/200] [Batch 304/469] [D loss: 0.497341] [G loss: 1.178244]\n",
      "[Epoch 1/200] [Batch 305/469] [D loss: 0.560896] [G loss: 0.606373]\n",
      "[Epoch 1/200] [Batch 306/469] [D loss: 0.520657] [G loss: 1.391598]\n",
      "[Epoch 1/200] [Batch 307/469] [D loss: 0.538472] [G loss: 0.540584]\n",
      "[Epoch 1/200] [Batch 308/469] [D loss: 0.535440] [G loss: 1.656711]\n",
      "[Epoch 1/200] [Batch 309/469] [D loss: 0.555675] [G loss: 0.500067]\n",
      "[Epoch 1/200] [Batch 310/469] [D loss: 0.491252] [G loss: 1.451230]\n",
      "[Epoch 1/200] [Batch 311/469] [D loss: 0.443190] [G loss: 0.748866]\n",
      "[Epoch 1/200] [Batch 312/469] [D loss: 0.419722] [G loss: 1.236170]\n",
      "[Epoch 1/200] [Batch 313/469] [D loss: 0.447261] [G loss: 0.840028]\n",
      "[Epoch 1/200] [Batch 314/469] [D loss: 0.456784] [G loss: 1.096211]\n",
      "[Epoch 1/200] [Batch 315/469] [D loss: 0.499814] [G loss: 0.738188]\n",
      "[Epoch 1/200] [Batch 316/469] [D loss: 0.531851] [G loss: 1.222732]\n",
      "[Epoch 1/200] [Batch 317/469] [D loss: 0.641682] [G loss: 0.414569]\n",
      "[Epoch 1/200] [Batch 318/469] [D loss: 0.711471] [G loss: 1.865760]\n",
      "[Epoch 1/200] [Batch 319/469] [D loss: 0.866724] [G loss: 0.225139]\n",
      "[Epoch 1/200] [Batch 320/469] [D loss: 0.574203] [G loss: 1.421273]\n",
      "[Epoch 1/200] [Batch 321/469] [D loss: 0.492029] [G loss: 0.771056]\n",
      "[Epoch 1/200] [Batch 322/469] [D loss: 0.456100] [G loss: 0.876243]\n",
      "[Epoch 1/200] [Batch 323/469] [D loss: 0.455765] [G loss: 1.164364]\n",
      "[Epoch 1/200] [Batch 324/469] [D loss: 0.468788] [G loss: 0.746815]\n",
      "[Epoch 1/200] [Batch 325/469] [D loss: 0.462138] [G loss: 1.226115]\n",
      "[Epoch 1/200] [Batch 326/469] [D loss: 0.464347] [G loss: 0.766946]\n",
      "[Epoch 1/200] [Batch 327/469] [D loss: 0.469612] [G loss: 1.239321]\n",
      "[Epoch 1/200] [Batch 328/469] [D loss: 0.503942] [G loss: 0.637952]\n",
      "[Epoch 1/200] [Batch 329/469] [D loss: 0.488718] [G loss: 1.370516]\n",
      "[Epoch 1/200] [Batch 330/469] [D loss: 0.522244] [G loss: 0.589542]\n",
      "[Epoch 1/200] [Batch 331/469] [D loss: 0.537001] [G loss: 1.416581]\n",
      "[Epoch 1/200] [Batch 332/469] [D loss: 0.530328] [G loss: 0.531469]\n",
      "[Epoch 1/200] [Batch 333/469] [D loss: 0.498842] [G loss: 1.699354]\n",
      "[Epoch 1/200] [Batch 334/469] [D loss: 0.481785] [G loss: 0.638127]\n",
      "[Epoch 1/200] [Batch 335/469] [D loss: 0.410796] [G loss: 1.274644]\n",
      "[Epoch 1/200] [Batch 336/469] [D loss: 0.429405] [G loss: 0.987788]\n",
      "[Epoch 1/200] [Batch 337/469] [D loss: 0.450390] [G loss: 0.872341]\n",
      "[Epoch 1/200] [Batch 338/469] [D loss: 0.486277] [G loss: 1.190112]\n",
      "[Epoch 1/200] [Batch 339/469] [D loss: 0.545423] [G loss: 0.583243]\n",
      "[Epoch 1/200] [Batch 340/469] [D loss: 0.567817] [G loss: 1.674805]\n",
      "[Epoch 1/200] [Batch 341/469] [D loss: 0.678314] [G loss: 0.354001]\n",
      "[Epoch 1/200] [Batch 342/469] [D loss: 0.511396] [G loss: 1.801495]\n",
      "[Epoch 1/200] [Batch 343/469] [D loss: 0.476376] [G loss: 0.689082]\n",
      "[Epoch 1/200] [Batch 344/469] [D loss: 0.384451] [G loss: 1.242333]\n",
      "[Epoch 1/200] [Batch 345/469] [D loss: 0.372489] [G loss: 1.202197]\n",
      "[Epoch 1/200] [Batch 346/469] [D loss: 0.418868] [G loss: 0.963347]\n",
      "[Epoch 1/200] [Batch 347/469] [D loss: 0.411897] [G loss: 1.171204]\n",
      "[Epoch 1/200] [Batch 348/469] [D loss: 0.453579] [G loss: 0.935674]\n",
      "[Epoch 1/200] [Batch 349/469] [D loss: 0.459945] [G loss: 1.062443]\n",
      "[Epoch 1/200] [Batch 350/469] [D loss: 0.478373] [G loss: 0.855838]\n",
      "[Epoch 1/200] [Batch 351/469] [D loss: 0.485901] [G loss: 1.283723]\n",
      "[Epoch 1/200] [Batch 352/469] [D loss: 0.573233] [G loss: 0.512230]\n",
      "[Epoch 1/200] [Batch 353/469] [D loss: 0.746445] [G loss: 2.436148]\n",
      "[Epoch 1/200] [Batch 354/469] [D loss: 0.966489] [G loss: 0.175369]\n",
      "[Epoch 1/200] [Batch 355/469] [D loss: 0.474480] [G loss: 1.767404]\n",
      "[Epoch 1/200] [Batch 356/469] [D loss: 0.421803] [G loss: 1.132969]\n",
      "[Epoch 1/200] [Batch 357/469] [D loss: 0.443862] [G loss: 0.773397]\n",
      "[Epoch 1/200] [Batch 358/469] [D loss: 0.453479] [G loss: 1.541376]\n",
      "[Epoch 1/200] [Batch 359/469] [D loss: 0.487982] [G loss: 0.701478]\n",
      "[Epoch 1/200] [Batch 360/469] [D loss: 0.483519] [G loss: 1.432718]\n",
      "[Epoch 1/200] [Batch 361/469] [D loss: 0.492101] [G loss: 0.668481]\n",
      "[Epoch 1/200] [Batch 362/469] [D loss: 0.485615] [G loss: 1.568354]\n",
      "[Epoch 1/200] [Batch 363/469] [D loss: 0.527879] [G loss: 0.612067]\n",
      "[Epoch 1/200] [Batch 364/469] [D loss: 0.443727] [G loss: 1.354966]\n",
      "[Epoch 1/200] [Batch 365/469] [D loss: 0.425975] [G loss: 0.940012]\n",
      "[Epoch 1/200] [Batch 366/469] [D loss: 0.422701] [G loss: 1.063289]\n",
      "[Epoch 1/200] [Batch 367/469] [D loss: 0.399552] [G loss: 1.144858]\n",
      "[Epoch 1/200] [Batch 368/469] [D loss: 0.396349] [G loss: 1.052436]\n",
      "[Epoch 1/200] [Batch 369/469] [D loss: 0.395332] [G loss: 1.180986]\n",
      "[Epoch 1/200] [Batch 370/469] [D loss: 0.406978] [G loss: 0.999577]\n",
      "[Epoch 1/200] [Batch 371/469] [D loss: 0.398743] [G loss: 1.196707]\n",
      "[Epoch 1/200] [Batch 372/469] [D loss: 0.394114] [G loss: 1.070584]\n",
      "[Epoch 1/200] [Batch 373/469] [D loss: 0.401765] [G loss: 1.182597]\n",
      "[Epoch 1/200] [Batch 374/469] [D loss: 0.419982] [G loss: 0.995251]\n",
      "[Epoch 1/200] [Batch 375/469] [D loss: 0.448326] [G loss: 1.257490]\n",
      "[Epoch 1/200] [Batch 376/469] [D loss: 0.481182] [G loss: 0.704912]\n",
      "[Epoch 1/200] [Batch 377/469] [D loss: 0.563669] [G loss: 2.149353]\n",
      "[Epoch 1/200] [Batch 378/469] [D loss: 0.934563] [G loss: 0.188898]\n",
      "[Epoch 1/200] [Batch 379/469] [D loss: 0.669054] [G loss: 2.855285]\n",
      "[Epoch 1/200] [Batch 380/469] [D loss: 0.487321] [G loss: 0.610681]\n",
      "[Epoch 1/200] [Batch 381/469] [D loss: 0.324546] [G loss: 1.431251]\n",
      "[Epoch 1/200] [Batch 382/469] [D loss: 0.335480] [G loss: 1.551790]\n",
      "[Epoch 1/200] [Batch 383/469] [D loss: 0.426085] [G loss: 0.766240]\n",
      "[Epoch 1/200] [Batch 384/469] [D loss: 0.421821] [G loss: 1.623360]\n",
      "[Epoch 1/200] [Batch 385/469] [D loss: 0.517375] [G loss: 0.624374]\n",
      "[Epoch 1/200] [Batch 386/469] [D loss: 0.554362] [G loss: 1.758427]\n",
      "[Epoch 1/200] [Batch 387/469] [D loss: 0.706965] [G loss: 0.349823]\n",
      "[Epoch 1/200] [Batch 388/469] [D loss: 0.621500] [G loss: 2.185952]\n",
      "[Epoch 1/200] [Batch 389/469] [D loss: 0.609835] [G loss: 0.497678]\n",
      "[Epoch 1/200] [Batch 390/469] [D loss: 0.423610] [G loss: 1.637332]\n",
      "[Epoch 1/200] [Batch 391/469] [D loss: 0.376809] [G loss: 1.049494]\n",
      "[Epoch 1/200] [Batch 392/469] [D loss: 0.371149] [G loss: 1.067575]\n",
      "[Epoch 1/200] [Batch 393/469] [D loss: 0.374113] [G loss: 1.344046]\n",
      "[Epoch 1/200] [Batch 394/469] [D loss: 0.399462] [G loss: 0.966939]\n",
      "[Epoch 1/200] [Batch 395/469] [D loss: 0.409194] [G loss: 1.382644]\n",
      "[Epoch 1/200] [Batch 396/469] [D loss: 0.417541] [G loss: 0.865722]\n",
      "[Epoch 1/200] [Batch 397/469] [D loss: 0.459263] [G loss: 1.601879]\n",
      "[Epoch 1/200] [Batch 398/469] [D loss: 0.557003] [G loss: 0.514379]\n",
      "[Epoch 1/200] [Batch 399/469] [D loss: 0.494022] [G loss: 2.061507]\n",
      "[Epoch 1/200] [Batch 400/469] [D loss: 0.512050] [G loss: 0.575397]\n",
      "[Epoch 1/200] [Batch 401/469] [D loss: 0.400843] [G loss: 1.661301]\n",
      "[Epoch 1/200] [Batch 402/469] [D loss: 0.409135] [G loss: 0.911865]\n",
      "[Epoch 1/200] [Batch 403/469] [D loss: 0.387907] [G loss: 1.197937]\n",
      "[Epoch 1/200] [Batch 404/469] [D loss: 0.359321] [G loss: 1.170264]\n",
      "[Epoch 1/200] [Batch 405/469] [D loss: 0.421750] [G loss: 1.213205]\n",
      "[Epoch 1/200] [Batch 406/469] [D loss: 0.408233] [G loss: 0.824364]\n",
      "[Epoch 1/200] [Batch 407/469] [D loss: 0.392654] [G loss: 1.887294]\n",
      "[Epoch 1/200] [Batch 408/469] [D loss: 0.426269] [G loss: 0.757407]\n",
      "[Epoch 1/200] [Batch 409/469] [D loss: 0.351566] [G loss: 1.710995]\n",
      "[Epoch 1/200] [Batch 410/469] [D loss: 0.376305] [G loss: 0.988057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/200] [Batch 411/469] [D loss: 0.354339] [G loss: 1.245337]\n",
      "[Epoch 1/200] [Batch 412/469] [D loss: 0.377988] [G loss: 1.142257]\n",
      "[Epoch 1/200] [Batch 413/469] [D loss: 0.359048] [G loss: 1.168160]\n",
      "[Epoch 1/200] [Batch 414/469] [D loss: 0.344671] [G loss: 1.251360]\n",
      "[Epoch 1/200] [Batch 415/469] [D loss: 0.349794] [G loss: 1.154816]\n",
      "[Epoch 1/200] [Batch 416/469] [D loss: 0.338367] [G loss: 1.281850]\n",
      "[Epoch 1/200] [Batch 417/469] [D loss: 0.348547] [G loss: 1.206413]\n",
      "[Epoch 1/200] [Batch 418/469] [D loss: 0.358071] [G loss: 1.189598]\n",
      "[Epoch 1/200] [Batch 419/469] [D loss: 0.340645] [G loss: 1.174511]\n",
      "[Epoch 1/200] [Batch 420/469] [D loss: 0.305204] [G loss: 1.338374]\n",
      "[Epoch 1/200] [Batch 421/469] [D loss: 0.306329] [G loss: 1.228981]\n",
      "[Epoch 1/200] [Batch 422/469] [D loss: 0.306355] [G loss: 1.321090]\n",
      "[Epoch 1/200] [Batch 423/469] [D loss: 0.318381] [G loss: 1.181433]\n",
      "[Epoch 1/200] [Batch 424/469] [D loss: 0.315354] [G loss: 1.311347]\n",
      "[Epoch 1/200] [Batch 425/469] [D loss: 0.305253] [G loss: 1.135924]\n",
      "[Epoch 1/200] [Batch 426/469] [D loss: 0.319013] [G loss: 1.511257]\n",
      "[Epoch 1/200] [Batch 427/469] [D loss: 0.360941] [G loss: 0.833088]\n",
      "[Epoch 1/200] [Batch 428/469] [D loss: 0.477049] [G loss: 1.955552]\n",
      "[Epoch 1/200] [Batch 429/469] [D loss: 0.720325] [G loss: 0.361918]\n",
      "[Epoch 1/200] [Batch 430/469] [D loss: 0.494802] [G loss: 2.338416]\n",
      "[Epoch 1/200] [Batch 431/469] [D loss: 0.347003] [G loss: 0.862425]\n",
      "[Epoch 1/200] [Batch 432/469] [D loss: 0.268534] [G loss: 1.197149]\n",
      "[Epoch 1/200] [Batch 433/469] [D loss: 0.336914] [G loss: 1.746290]\n",
      "[Epoch 1/200] [Batch 434/469] [D loss: 0.393016] [G loss: 0.737285]\n",
      "[Epoch 1/200] [Batch 435/469] [D loss: 0.327998] [G loss: 1.707892]\n",
      "[Epoch 1/200] [Batch 436/469] [D loss: 0.357340] [G loss: 0.965386]\n",
      "[Epoch 1/200] [Batch 437/469] [D loss: 0.337229] [G loss: 1.306797]\n",
      "[Epoch 1/200] [Batch 438/469] [D loss: 0.376561] [G loss: 0.949140]\n",
      "[Epoch 1/200] [Batch 439/469] [D loss: 0.377706] [G loss: 1.296918]\n",
      "[Epoch 1/200] [Batch 440/469] [D loss: 0.409225] [G loss: 0.842510]\n",
      "[Epoch 1/200] [Batch 441/469] [D loss: 0.389275] [G loss: 1.514225]\n",
      "[Epoch 1/200] [Batch 442/469] [D loss: 0.455997] [G loss: 0.712076]\n",
      "[Epoch 1/200] [Batch 443/469] [D loss: 0.447506] [G loss: 1.874444]\n",
      "[Epoch 1/200] [Batch 444/469] [D loss: 0.618262] [G loss: 0.416307]\n",
      "[Epoch 1/200] [Batch 445/469] [D loss: 0.600704] [G loss: 2.624092]\n",
      "[Epoch 1/200] [Batch 446/469] [D loss: 0.539237] [G loss: 0.487055]\n",
      "[Epoch 1/200] [Batch 447/469] [D loss: 0.285341] [G loss: 1.673695]\n",
      "[Epoch 1/200] [Batch 448/469] [D loss: 0.330216] [G loss: 1.696945]\n",
      "[Epoch 1/200] [Batch 449/469] [D loss: 0.440482] [G loss: 0.695023]\n",
      "[Epoch 1/200] [Batch 450/469] [D loss: 0.376614] [G loss: 2.008455]\n",
      "[Epoch 1/200] [Batch 451/469] [D loss: 0.429237] [G loss: 0.798713]\n",
      "[Epoch 1/200] [Batch 452/469] [D loss: 0.416976] [G loss: 1.617935]\n",
      "[Epoch 1/200] [Batch 453/469] [D loss: 0.440201] [G loss: 0.774632]\n",
      "[Epoch 1/200] [Batch 454/469] [D loss: 0.492545] [G loss: 1.889797]\n",
      "[Epoch 1/200] [Batch 455/469] [D loss: 0.637777] [G loss: 0.401563]\n",
      "[Epoch 1/200] [Batch 456/469] [D loss: 0.744546] [G loss: 2.768347]\n",
      "[Epoch 1/200] [Batch 457/469] [D loss: 0.832197] [G loss: 0.252941]\n",
      "[Epoch 1/200] [Batch 458/469] [D loss: 0.388990] [G loss: 1.801096]\n",
      "[Epoch 1/200] [Batch 459/469] [D loss: 0.350974] [G loss: 1.417089]\n",
      "[Epoch 1/200] [Batch 460/469] [D loss: 0.433114] [G loss: 0.752267]\n",
      "[Epoch 1/200] [Batch 461/469] [D loss: 0.454497] [G loss: 1.820282]\n",
      "[Epoch 1/200] [Batch 462/469] [D loss: 0.533435] [G loss: 0.600467]\n",
      "[Epoch 1/200] [Batch 463/469] [D loss: 0.532845] [G loss: 2.038316]\n",
      "[Epoch 1/200] [Batch 464/469] [D loss: 0.577050] [G loss: 0.512285]\n",
      "[Epoch 1/200] [Batch 465/469] [D loss: 0.499190] [G loss: 2.058963]\n",
      "[Epoch 1/200] [Batch 466/469] [D loss: 0.478150] [G loss: 0.611069]\n",
      "[Epoch 1/200] [Batch 467/469] [D loss: 0.380476] [G loss: 2.009606]\n",
      "[Epoch 1/200] [Batch 468/469] [D loss: 0.389133] [G loss: 0.973742]\n",
      "[Epoch 2/200] [Batch 0/469] [D loss: 0.346589] [G loss: 1.317258]\n",
      "[Epoch 2/200] [Batch 1/469] [D loss: 0.378992] [G loss: 1.344392]\n",
      "[Epoch 2/200] [Batch 2/469] [D loss: 0.425201] [G loss: 0.856351]\n",
      "[Epoch 2/200] [Batch 3/469] [D loss: 0.371306] [G loss: 1.797284]\n",
      "[Epoch 2/200] [Batch 4/469] [D loss: 0.434947] [G loss: 0.847386]\n",
      "[Epoch 2/200] [Batch 5/469] [D loss: 0.449209] [G loss: 1.861200]\n",
      "[Epoch 2/200] [Batch 6/469] [D loss: 0.532133] [G loss: 0.632074]\n",
      "[Epoch 2/200] [Batch 7/469] [D loss: 0.491962] [G loss: 2.145346]\n",
      "[Epoch 2/200] [Batch 8/469] [D loss: 0.546251] [G loss: 0.526786]\n",
      "[Epoch 2/200] [Batch 9/469] [D loss: 0.449220] [G loss: 2.362552]\n",
      "[Epoch 2/200] [Batch 10/469] [D loss: 0.404388] [G loss: 0.830436]\n",
      "[Epoch 2/200] [Batch 11/469] [D loss: 0.357188] [G loss: 1.608548]\n",
      "[Epoch 2/200] [Batch 12/469] [D loss: 0.341885] [G loss: 1.099698]\n",
      "[Epoch 2/200] [Batch 13/469] [D loss: 0.357832] [G loss: 1.431639]\n",
      "[Epoch 2/200] [Batch 14/469] [D loss: 0.375135] [G loss: 1.090500]\n",
      "[Epoch 2/200] [Batch 15/469] [D loss: 0.379319] [G loss: 1.325405]\n",
      "[Epoch 2/200] [Batch 16/469] [D loss: 0.377011] [G loss: 1.085194]\n",
      "[Epoch 2/200] [Batch 17/469] [D loss: 0.362015] [G loss: 1.548389]\n",
      "[Epoch 2/200] [Batch 18/469] [D loss: 0.402414] [G loss: 0.851694]\n",
      "[Epoch 2/200] [Batch 19/469] [D loss: 0.395055] [G loss: 2.240711]\n",
      "[Epoch 2/200] [Batch 20/469] [D loss: 0.491933] [G loss: 0.606748]\n",
      "[Epoch 2/200] [Batch 21/469] [D loss: 0.433993] [G loss: 2.788448]\n",
      "[Epoch 2/200] [Batch 22/469] [D loss: 0.399512] [G loss: 0.810244]\n",
      "[Epoch 2/200] [Batch 23/469] [D loss: 0.272245] [G loss: 1.896926]\n",
      "[Epoch 2/200] [Batch 24/469] [D loss: 0.312200] [G loss: 1.502989]\n",
      "[Epoch 2/200] [Batch 25/469] [D loss: 0.392209] [G loss: 0.833740]\n",
      "[Epoch 2/200] [Batch 26/469] [D loss: 0.469782] [G loss: 2.240724]\n",
      "[Epoch 2/200] [Batch 27/469] [D loss: 0.599276] [G loss: 0.452465]\n",
      "[Epoch 2/200] [Batch 28/469] [D loss: 0.440250] [G loss: 2.615005]\n",
      "[Epoch 2/200] [Batch 29/469] [D loss: 0.366718] [G loss: 0.972625]\n",
      "[Epoch 2/200] [Batch 30/469] [D loss: 0.298628] [G loss: 1.486626]\n",
      "[Epoch 2/200] [Batch 31/469] [D loss: 0.321788] [G loss: 1.431418]\n",
      "[Epoch 2/200] [Batch 32/469] [D loss: 0.366941] [G loss: 1.115602]\n",
      "[Epoch 2/200] [Batch 33/469] [D loss: 0.343554] [G loss: 1.231426]\n",
      "[Epoch 2/200] [Batch 34/469] [D loss: 0.380463] [G loss: 1.288033]\n",
      "[Epoch 2/200] [Batch 35/469] [D loss: 0.430615] [G loss: 0.882771]\n",
      "[Epoch 2/200] [Batch 36/469] [D loss: 0.372882] [G loss: 1.420826]\n",
      "[Epoch 2/200] [Batch 37/469] [D loss: 0.377434] [G loss: 0.906199]\n",
      "[Epoch 2/200] [Batch 38/469] [D loss: 0.431576] [G loss: 1.628208]\n",
      "[Epoch 2/200] [Batch 39/469] [D loss: 0.478205] [G loss: 0.617348]\n",
      "[Epoch 2/200] [Batch 40/469] [D loss: 0.431217] [G loss: 2.171597]\n",
      "[Epoch 2/200] [Batch 41/469] [D loss: 0.431711] [G loss: 0.749723]\n",
      "[Epoch 2/200] [Batch 42/469] [D loss: 0.281195] [G loss: 1.510315]\n",
      "[Epoch 2/200] [Batch 43/469] [D loss: 0.336288] [G loss: 1.511234]\n",
      "[Epoch 2/200] [Batch 44/469] [D loss: 0.414982] [G loss: 0.801227]\n",
      "[Epoch 2/200] [Batch 45/469] [D loss: 0.413915] [G loss: 1.727635]\n",
      "[Epoch 2/200] [Batch 46/469] [D loss: 0.407829] [G loss: 0.750349]\n",
      "[Epoch 2/200] [Batch 47/469] [D loss: 0.411904] [G loss: 1.947270]\n",
      "[Epoch 2/200] [Batch 48/469] [D loss: 0.466961] [G loss: 0.638796]\n",
      "[Epoch 2/200] [Batch 49/469] [D loss: 0.375760] [G loss: 1.818812]\n",
      "[Epoch 2/200] [Batch 50/469] [D loss: 0.349928] [G loss: 0.982867]\n",
      "[Epoch 2/200] [Batch 51/469] [D loss: 0.339576] [G loss: 1.441479]\n",
      "[Epoch 2/200] [Batch 52/469] [D loss: 0.376075] [G loss: 1.125251]\n",
      "[Epoch 2/200] [Batch 53/469] [D loss: 0.371876] [G loss: 1.077315]\n",
      "[Epoch 2/200] [Batch 54/469] [D loss: 0.385096] [G loss: 1.263657]\n",
      "[Epoch 2/200] [Batch 55/469] [D loss: 0.390758] [G loss: 0.914675]\n",
      "[Epoch 2/200] [Batch 56/469] [D loss: 0.396306] [G loss: 1.499505]\n",
      "[Epoch 2/200] [Batch 57/469] [D loss: 0.520048] [G loss: 0.626077]\n",
      "[Epoch 2/200] [Batch 58/469] [D loss: 0.569131] [G loss: 2.220163]\n",
      "[Epoch 2/200] [Batch 59/469] [D loss: 0.748612] [G loss: 0.277148]\n",
      "[Epoch 2/200] [Batch 60/469] [D loss: 0.474586] [G loss: 2.711185]\n",
      "[Epoch 2/200] [Batch 61/469] [D loss: 0.362018] [G loss: 0.849220]\n",
      "[Epoch 2/200] [Batch 62/469] [D loss: 0.272325] [G loss: 1.461132]\n",
      "[Epoch 2/200] [Batch 63/469] [D loss: 0.291871] [G loss: 1.700859]\n",
      "[Epoch 2/200] [Batch 64/469] [D loss: 0.349122] [G loss: 1.107428]\n",
      "[Epoch 2/200] [Batch 65/469] [D loss: 0.375855] [G loss: 1.455799]\n",
      "[Epoch 2/200] [Batch 66/469] [D loss: 0.425376] [G loss: 1.026101]\n",
      "[Epoch 2/200] [Batch 67/469] [D loss: 0.435075] [G loss: 1.248415]\n",
      "[Epoch 2/200] [Batch 68/469] [D loss: 0.438641] [G loss: 0.928131]\n",
      "[Epoch 2/200] [Batch 69/469] [D loss: 0.461658] [G loss: 1.820482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 70/469] [D loss: 0.657250] [G loss: 0.418053]\n",
      "[Epoch 2/200] [Batch 71/469] [D loss: 0.840408] [G loss: 3.383815]\n",
      "[Epoch 2/200] [Batch 72/469] [D loss: 1.057527] [G loss: 0.183389]\n",
      "[Epoch 2/200] [Batch 73/469] [D loss: 0.350616] [G loss: 2.185173]\n",
      "[Epoch 2/200] [Batch 74/469] [D loss: 0.384687] [G loss: 2.326027]\n",
      "[Epoch 2/200] [Batch 75/469] [D loss: 0.492040] [G loss: 0.707381]\n",
      "[Epoch 2/200] [Batch 76/469] [D loss: 0.361821] [G loss: 1.874710]\n",
      "[Epoch 2/200] [Batch 77/469] [D loss: 0.339871] [G loss: 1.176298]\n",
      "[Epoch 2/200] [Batch 78/469] [D loss: 0.382077] [G loss: 1.230598]\n",
      "[Epoch 2/200] [Batch 79/469] [D loss: 0.425638] [G loss: 1.219650]\n",
      "[Epoch 2/200] [Batch 80/469] [D loss: 0.489615] [G loss: 0.953088]\n",
      "[Epoch 2/200] [Batch 81/469] [D loss: 0.473998] [G loss: 1.185665]\n",
      "[Epoch 2/200] [Batch 82/469] [D loss: 0.499102] [G loss: 0.850977]\n",
      "[Epoch 2/200] [Batch 83/469] [D loss: 0.499016] [G loss: 1.571013]\n",
      "[Epoch 2/200] [Batch 84/469] [D loss: 0.643658] [G loss: 0.453486]\n",
      "[Epoch 2/200] [Batch 85/469] [D loss: 0.785132] [G loss: 2.776823]\n",
      "[Epoch 2/200] [Batch 86/469] [D loss: 0.850759] [G loss: 0.251835]\n",
      "[Epoch 2/200] [Batch 87/469] [D loss: 0.385907] [G loss: 2.162476]\n",
      "[Epoch 2/200] [Batch 88/469] [D loss: 0.320474] [G loss: 1.668695]\n",
      "[Epoch 2/200] [Batch 89/469] [D loss: 0.345766] [G loss: 0.996931]\n",
      "[Epoch 2/200] [Batch 90/469] [D loss: 0.350986] [G loss: 1.795487]\n",
      "[Epoch 2/200] [Batch 91/469] [D loss: 0.363401] [G loss: 0.915722]\n",
      "[Epoch 2/200] [Batch 92/469] [D loss: 0.371129] [G loss: 1.515169]\n",
      "[Epoch 2/200] [Batch 93/469] [D loss: 0.436044] [G loss: 0.910671]\n",
      "[Epoch 2/200] [Batch 94/469] [D loss: 0.390037] [G loss: 1.324975]\n",
      "[Epoch 2/200] [Batch 95/469] [D loss: 0.407166] [G loss: 1.132824]\n",
      "[Epoch 2/200] [Batch 96/469] [D loss: 0.439653] [G loss: 1.114676]\n",
      "[Epoch 2/200] [Batch 97/469] [D loss: 0.438424] [G loss: 0.973370]\n",
      "[Epoch 2/200] [Batch 98/469] [D loss: 0.486218] [G loss: 1.412436]\n",
      "[Epoch 2/200] [Batch 99/469] [D loss: 0.536493] [G loss: 0.562317]\n",
      "[Epoch 2/200] [Batch 100/469] [D loss: 0.575687] [G loss: 2.488411]\n",
      "[Epoch 2/200] [Batch 101/469] [D loss: 0.616042] [G loss: 0.416675]\n",
      "[Epoch 2/200] [Batch 102/469] [D loss: 0.386985] [G loss: 2.284792]\n",
      "[Epoch 2/200] [Batch 103/469] [D loss: 0.327484] [G loss: 1.178266]\n",
      "[Epoch 2/200] [Batch 104/469] [D loss: 0.325471] [G loss: 1.062473]\n",
      "[Epoch 2/200] [Batch 105/469] [D loss: 0.371986] [G loss: 1.795015]\n",
      "[Epoch 2/200] [Batch 106/469] [D loss: 0.436854] [G loss: 0.742528]\n",
      "[Epoch 2/200] [Batch 107/469] [D loss: 0.437968] [G loss: 1.685838]\n",
      "[Epoch 2/200] [Batch 108/469] [D loss: 0.504118] [G loss: 0.621955]\n",
      "[Epoch 2/200] [Batch 109/469] [D loss: 0.536099] [G loss: 1.961446]\n",
      "[Epoch 2/200] [Batch 110/469] [D loss: 0.643990] [G loss: 0.374887]\n",
      "[Epoch 2/200] [Batch 111/469] [D loss: 0.501566] [G loss: 2.427757]\n",
      "[Epoch 2/200] [Batch 112/469] [D loss: 0.394401] [G loss: 0.793629]\n",
      "[Epoch 2/200] [Batch 113/469] [D loss: 0.300549] [G loss: 1.342809]\n",
      "[Epoch 2/200] [Batch 114/469] [D loss: 0.353191] [G loss: 1.632101]\n",
      "[Epoch 2/200] [Batch 115/469] [D loss: 0.399772] [G loss: 0.760479]\n",
      "[Epoch 2/200] [Batch 116/469] [D loss: 0.396259] [G loss: 2.004930]\n",
      "[Epoch 2/200] [Batch 117/469] [D loss: 0.442389] [G loss: 0.702428]\n",
      "[Epoch 2/200] [Batch 118/469] [D loss: 0.375353] [G loss: 1.831635]\n",
      "[Epoch 2/200] [Batch 119/469] [D loss: 0.402688] [G loss: 0.868705]\n",
      "[Epoch 2/200] [Batch 120/469] [D loss: 0.409587] [G loss: 1.449113]\n",
      "[Epoch 2/200] [Batch 121/469] [D loss: 0.442235] [G loss: 0.747071]\n",
      "[Epoch 2/200] [Batch 122/469] [D loss: 0.473698] [G loss: 2.007835]\n",
      "[Epoch 2/200] [Batch 123/469] [D loss: 0.603703] [G loss: 0.418986]\n",
      "[Epoch 2/200] [Batch 124/469] [D loss: 0.579636] [G loss: 2.667644]\n",
      "[Epoch 2/200] [Batch 125/469] [D loss: 0.516351] [G loss: 0.522075]\n",
      "[Epoch 2/200] [Batch 126/469] [D loss: 0.366565] [G loss: 1.746336]\n",
      "[Epoch 2/200] [Batch 127/469] [D loss: 0.348368] [G loss: 1.021844]\n",
      "[Epoch 2/200] [Batch 128/469] [D loss: 0.374570] [G loss: 1.193345]\n",
      "[Epoch 2/200] [Batch 129/469] [D loss: 0.438865] [G loss: 1.182096]\n",
      "[Epoch 2/200] [Batch 130/469] [D loss: 0.431956] [G loss: 0.858683]\n",
      "[Epoch 2/200] [Batch 131/469] [D loss: 0.450487] [G loss: 1.654987]\n",
      "[Epoch 2/200] [Batch 132/469] [D loss: 0.554109] [G loss: 0.535968]\n",
      "[Epoch 2/200] [Batch 133/469] [D loss: 0.498392] [G loss: 2.205989]\n",
      "[Epoch 2/200] [Batch 134/469] [D loss: 0.491120] [G loss: 0.589529]\n",
      "[Epoch 2/200] [Batch 135/469] [D loss: 0.371264] [G loss: 1.795772]\n",
      "[Epoch 2/200] [Batch 136/469] [D loss: 0.330067] [G loss: 1.092332]\n",
      "[Epoch 2/200] [Batch 137/469] [D loss: 0.303275] [G loss: 1.245790]\n",
      "[Epoch 2/200] [Batch 138/469] [D loss: 0.290138] [G loss: 1.446876]\n",
      "[Epoch 2/200] [Batch 139/469] [D loss: 0.336000] [G loss: 1.197217]\n",
      "[Epoch 2/200] [Batch 140/469] [D loss: 0.350548] [G loss: 1.080821]\n",
      "[Epoch 2/200] [Batch 141/469] [D loss: 0.392051] [G loss: 1.431523]\n",
      "[Epoch 2/200] [Batch 142/469] [D loss: 0.466871] [G loss: 0.763425]\n",
      "[Epoch 2/200] [Batch 143/469] [D loss: 0.413642] [G loss: 1.578264]\n",
      "[Epoch 2/200] [Batch 144/469] [D loss: 0.449051] [G loss: 0.755359]\n",
      "[Epoch 2/200] [Batch 145/469] [D loss: 0.373636] [G loss: 1.584786]\n",
      "[Epoch 2/200] [Batch 146/469] [D loss: 0.411632] [G loss: 0.910133]\n",
      "[Epoch 2/200] [Batch 147/469] [D loss: 0.333470] [G loss: 1.315319]\n",
      "[Epoch 2/200] [Batch 148/469] [D loss: 0.374270] [G loss: 1.348495]\n",
      "[Epoch 2/200] [Batch 149/469] [D loss: 0.361039] [G loss: 0.980149]\n",
      "[Epoch 2/200] [Batch 150/469] [D loss: 0.340915] [G loss: 1.563615]\n",
      "[Epoch 2/200] [Batch 151/469] [D loss: 0.343293] [G loss: 0.983021]\n",
      "[Epoch 2/200] [Batch 152/469] [D loss: 0.354189] [G loss: 1.585675]\n",
      "[Epoch 2/200] [Batch 153/469] [D loss: 0.378807] [G loss: 0.920309]\n",
      "[Epoch 2/200] [Batch 154/469] [D loss: 0.395802] [G loss: 1.546455]\n",
      "[Epoch 2/200] [Batch 155/469] [D loss: 0.467254] [G loss: 0.665535]\n",
      "[Epoch 2/200] [Batch 156/469] [D loss: 0.535311] [G loss: 2.201239]\n",
      "[Epoch 2/200] [Batch 157/469] [D loss: 0.613834] [G loss: 0.429750]\n",
      "[Epoch 2/200] [Batch 158/469] [D loss: 0.349183] [G loss: 2.127381]\n",
      "[Epoch 2/200] [Batch 159/469] [D loss: 0.287686] [G loss: 1.247734]\n",
      "[Epoch 2/200] [Batch 160/469] [D loss: 0.324614] [G loss: 1.040145]\n",
      "[Epoch 2/200] [Batch 161/469] [D loss: 0.291980] [G loss: 1.686934]\n",
      "[Epoch 2/200] [Batch 162/469] [D loss: 0.312541] [G loss: 1.091571]\n",
      "[Epoch 2/200] [Batch 163/469] [D loss: 0.358836] [G loss: 1.527892]\n",
      "[Epoch 2/200] [Batch 164/469] [D loss: 0.421081] [G loss: 0.810969]\n",
      "[Epoch 2/200] [Batch 165/469] [D loss: 0.434046] [G loss: 1.838155]\n",
      "[Epoch 2/200] [Batch 166/469] [D loss: 0.596676] [G loss: 0.407991]\n",
      "[Epoch 2/200] [Batch 167/469] [D loss: 0.699297] [G loss: 3.081382]\n",
      "[Epoch 2/200] [Batch 168/469] [D loss: 0.830959] [G loss: 0.226187]\n",
      "[Epoch 2/200] [Batch 169/469] [D loss: 0.358632] [G loss: 2.316723]\n",
      "[Epoch 2/200] [Batch 170/469] [D loss: 0.280819] [G loss: 1.197205]\n",
      "[Epoch 2/200] [Batch 171/469] [D loss: 0.317857] [G loss: 1.201829]\n",
      "[Epoch 2/200] [Batch 172/469] [D loss: 0.370565] [G loss: 1.547129]\n",
      "[Epoch 2/200] [Batch 173/469] [D loss: 0.446594] [G loss: 0.698335]\n",
      "[Epoch 2/200] [Batch 174/469] [D loss: 0.528629] [G loss: 2.351237]\n",
      "[Epoch 2/200] [Batch 175/469] [D loss: 0.873200] [G loss: 0.217946]\n",
      "[Epoch 2/200] [Batch 176/469] [D loss: 0.670087] [G loss: 3.004700]\n",
      "[Epoch 2/200] [Batch 177/469] [D loss: 0.597281] [G loss: 0.467639]\n",
      "[Epoch 2/200] [Batch 178/469] [D loss: 0.363124] [G loss: 1.734089]\n",
      "[Epoch 2/200] [Batch 179/469] [D loss: 0.358221] [G loss: 1.148089]\n",
      "[Epoch 2/200] [Batch 180/469] [D loss: 0.409399] [G loss: 1.102170]\n",
      "[Epoch 2/200] [Batch 181/469] [D loss: 0.432054] [G loss: 1.047739]\n",
      "[Epoch 2/200] [Batch 182/469] [D loss: 0.437300] [G loss: 1.202889]\n",
      "[Epoch 2/200] [Batch 183/469] [D loss: 0.520995] [G loss: 0.837449]\n",
      "[Epoch 2/200] [Batch 184/469] [D loss: 0.447797] [G loss: 1.223480]\n",
      "[Epoch 2/200] [Batch 185/469] [D loss: 0.469139] [G loss: 0.883602]\n",
      "[Epoch 2/200] [Batch 186/469] [D loss: 0.450025] [G loss: 1.433526]\n",
      "[Epoch 2/200] [Batch 187/469] [D loss: 0.560467] [G loss: 0.527559]\n",
      "[Epoch 2/200] [Batch 188/469] [D loss: 0.772062] [G loss: 2.795675]\n",
      "[Epoch 2/200] [Batch 189/469] [D loss: 1.013318] [G loss: 0.168214]\n",
      "[Epoch 2/200] [Batch 190/469] [D loss: 0.543555] [G loss: 2.429003]\n",
      "[Epoch 2/200] [Batch 191/469] [D loss: 0.405280] [G loss: 0.866666]\n",
      "[Epoch 2/200] [Batch 192/469] [D loss: 0.363139] [G loss: 1.323691]\n",
      "[Epoch 2/200] [Batch 193/469] [D loss: 0.363990] [G loss: 1.246631]\n",
      "[Epoch 2/200] [Batch 194/469] [D loss: 0.369647] [G loss: 1.027478]\n",
      "[Epoch 2/200] [Batch 195/469] [D loss: 0.407240] [G loss: 1.475003]\n",
      "[Epoch 2/200] [Batch 196/469] [D loss: 0.439952] [G loss: 0.795824]\n",
      "[Epoch 2/200] [Batch 197/469] [D loss: 0.464434] [G loss: 1.746269]\n",
      "[Epoch 2/200] [Batch 198/469] [D loss: 0.500606] [G loss: 0.622113]\n",
      "[Epoch 2/200] [Batch 199/469] [D loss: 0.384705] [G loss: 1.967119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 200/469] [D loss: 0.372542] [G loss: 1.033501]\n",
      "[Epoch 2/200] [Batch 201/469] [D loss: 0.372064] [G loss: 1.129232]\n",
      "[Epoch 2/200] [Batch 202/469] [D loss: 0.378667] [G loss: 1.362943]\n",
      "[Epoch 2/200] [Batch 203/469] [D loss: 0.352146] [G loss: 1.011712]\n",
      "[Epoch 2/200] [Batch 204/469] [D loss: 0.367898] [G loss: 1.629767]\n",
      "[Epoch 2/200] [Batch 205/469] [D loss: 0.388695] [G loss: 0.881772]\n",
      "[Epoch 2/200] [Batch 206/469] [D loss: 0.389937] [G loss: 1.797219]\n",
      "[Epoch 2/200] [Batch 207/469] [D loss: 0.427859] [G loss: 0.774659]\n",
      "[Epoch 2/200] [Batch 208/469] [D loss: 0.380402] [G loss: 1.985776]\n",
      "[Epoch 2/200] [Batch 209/469] [D loss: 0.411522] [G loss: 0.761360]\n",
      "[Epoch 2/200] [Batch 210/469] [D loss: 0.400330] [G loss: 1.795785]\n",
      "[Epoch 2/200] [Batch 211/469] [D loss: 0.474648] [G loss: 0.738016]\n",
      "[Epoch 2/200] [Batch 212/469] [D loss: 0.425259] [G loss: 1.888831]\n",
      "[Epoch 2/200] [Batch 213/469] [D loss: 0.524491] [G loss: 0.583899]\n",
      "[Epoch 2/200] [Batch 214/469] [D loss: 0.476331] [G loss: 2.151135]\n",
      "[Epoch 2/200] [Batch 215/469] [D loss: 0.627950] [G loss: 0.445112]\n",
      "[Epoch 2/200] [Batch 216/469] [D loss: 0.569396] [G loss: 2.371158]\n",
      "[Epoch 2/200] [Batch 217/469] [D loss: 0.572746] [G loss: 0.468842]\n",
      "[Epoch 2/200] [Batch 218/469] [D loss: 0.396678] [G loss: 2.026999]\n",
      "[Epoch 2/200] [Batch 219/469] [D loss: 0.371384] [G loss: 1.002770]\n",
      "[Epoch 2/200] [Batch 220/469] [D loss: 0.342791] [G loss: 1.236640]\n",
      "[Epoch 2/200] [Batch 221/469] [D loss: 0.322678] [G loss: 1.352485]\n",
      "[Epoch 2/200] [Batch 222/469] [D loss: 0.385812] [G loss: 1.179414]\n",
      "[Epoch 2/200] [Batch 223/469] [D loss: 0.349223] [G loss: 1.046234]\n",
      "[Epoch 2/200] [Batch 224/469] [D loss: 0.367018] [G loss: 1.551069]\n",
      "[Epoch 2/200] [Batch 225/469] [D loss: 0.480921] [G loss: 0.713398]\n",
      "[Epoch 2/200] [Batch 226/469] [D loss: 0.479128] [G loss: 1.793359]\n",
      "[Epoch 2/200] [Batch 227/469] [D loss: 0.583656] [G loss: 0.452461]\n",
      "[Epoch 2/200] [Batch 228/469] [D loss: 0.504807] [G loss: 2.229150]\n",
      "[Epoch 2/200] [Batch 229/469] [D loss: 0.441254] [G loss: 0.641897]\n",
      "[Epoch 2/200] [Batch 230/469] [D loss: 0.276051] [G loss: 1.766959]\n",
      "[Epoch 2/200] [Batch 231/469] [D loss: 0.277559] [G loss: 1.473310]\n",
      "[Epoch 2/200] [Batch 232/469] [D loss: 0.339068] [G loss: 1.028231]\n",
      "[Epoch 2/200] [Batch 233/469] [D loss: 0.342796] [G loss: 1.585701]\n",
      "[Epoch 2/200] [Batch 234/469] [D loss: 0.371099] [G loss: 0.906582]\n",
      "[Epoch 2/200] [Batch 235/469] [D loss: 0.383913] [G loss: 1.742360]\n",
      "[Epoch 2/200] [Batch 236/469] [D loss: 0.460566] [G loss: 0.648576]\n",
      "[Epoch 2/200] [Batch 237/469] [D loss: 0.509504] [G loss: 2.326537]\n",
      "[Epoch 2/200] [Batch 238/469] [D loss: 0.647993] [G loss: 0.355750]\n",
      "[Epoch 2/200] [Batch 239/469] [D loss: 0.521736] [G loss: 2.773791]\n",
      "[Epoch 2/200] [Batch 240/469] [D loss: 0.399144] [G loss: 0.736800]\n",
      "[Epoch 2/200] [Batch 241/469] [D loss: 0.277480] [G loss: 1.619461]\n",
      "[Epoch 2/200] [Batch 242/469] [D loss: 0.286625] [G loss: 1.410131]\n",
      "[Epoch 2/200] [Batch 243/469] [D loss: 0.339088] [G loss: 1.081930]\n",
      "[Epoch 2/200] [Batch 244/469] [D loss: 0.363229] [G loss: 1.480178]\n",
      "[Epoch 2/200] [Batch 245/469] [D loss: 0.451820] [G loss: 0.812294]\n",
      "[Epoch 2/200] [Batch 246/469] [D loss: 0.458271] [G loss: 1.520326]\n",
      "[Epoch 2/200] [Batch 247/469] [D loss: 0.583368] [G loss: 0.460332]\n",
      "[Epoch 2/200] [Batch 248/469] [D loss: 0.737679] [G loss: 2.993596]\n",
      "[Epoch 2/200] [Batch 249/469] [D loss: 0.778757] [G loss: 0.267072]\n",
      "[Epoch 2/200] [Batch 250/469] [D loss: 0.284279] [G loss: 2.286857]\n",
      "[Epoch 2/200] [Batch 251/469] [D loss: 0.273966] [G loss: 1.841711]\n",
      "[Epoch 2/200] [Batch 252/469] [D loss: 0.364671] [G loss: 0.839480]\n",
      "[Epoch 2/200] [Batch 253/469] [D loss: 0.254044] [G loss: 2.068246]\n",
      "[Epoch 2/200] [Batch 254/469] [D loss: 0.286302] [G loss: 1.396070]\n",
      "[Epoch 2/200] [Batch 255/469] [D loss: 0.339914] [G loss: 1.042758]\n",
      "[Epoch 2/200] [Batch 256/469] [D loss: 0.335359] [G loss: 1.758893]\n",
      "[Epoch 2/200] [Batch 257/469] [D loss: 0.375614] [G loss: 0.798522]\n",
      "[Epoch 2/200] [Batch 258/469] [D loss: 0.366512] [G loss: 2.169086]\n",
      "[Epoch 2/200] [Batch 259/469] [D loss: 0.421001] [G loss: 0.669420]\n",
      "[Epoch 2/200] [Batch 260/469] [D loss: 0.338162] [G loss: 2.379317]\n",
      "[Epoch 2/200] [Batch 261/469] [D loss: 0.350715] [G loss: 0.815895]\n",
      "[Epoch 2/200] [Batch 262/469] [D loss: 0.324469] [G loss: 2.177186]\n",
      "[Epoch 2/200] [Batch 263/469] [D loss: 0.351135] [G loss: 0.801127]\n",
      "[Epoch 2/200] [Batch 264/469] [D loss: 0.365155] [G loss: 2.346331]\n",
      "[Epoch 2/200] [Batch 265/469] [D loss: 0.498709] [G loss: 0.526381]\n",
      "[Epoch 2/200] [Batch 266/469] [D loss: 0.432005] [G loss: 2.633595]\n",
      "[Epoch 2/200] [Batch 267/469] [D loss: 0.517942] [G loss: 0.507519]\n",
      "[Epoch 2/200] [Batch 268/469] [D loss: 0.465604] [G loss: 2.514734]\n",
      "[Epoch 2/200] [Batch 269/469] [D loss: 0.541976] [G loss: 0.475214]\n",
      "[Epoch 2/200] [Batch 270/469] [D loss: 0.448938] [G loss: 2.400052]\n",
      "[Epoch 2/200] [Batch 271/469] [D loss: 0.477178] [G loss: 0.586392]\n",
      "[Epoch 2/200] [Batch 272/469] [D loss: 0.401153] [G loss: 1.918372]\n",
      "[Epoch 2/200] [Batch 273/469] [D loss: 0.466665] [G loss: 0.661364]\n",
      "[Epoch 2/200] [Batch 274/469] [D loss: 0.418353] [G loss: 1.772280]\n",
      "[Epoch 2/200] [Batch 275/469] [D loss: 0.507078] [G loss: 0.567436]\n",
      "[Epoch 2/200] [Batch 276/469] [D loss: 0.498906] [G loss: 2.300385]\n",
      "[Epoch 2/200] [Batch 277/469] [D loss: 0.628594] [G loss: 0.400056]\n",
      "[Epoch 2/200] [Batch 278/469] [D loss: 0.451688] [G loss: 2.169155]\n",
      "[Epoch 2/200] [Batch 279/469] [D loss: 0.451395] [G loss: 0.681867]\n",
      "[Epoch 2/200] [Batch 280/469] [D loss: 0.388967] [G loss: 1.590857]\n",
      "[Epoch 2/200] [Batch 281/469] [D loss: 0.435175] [G loss: 0.933342]\n",
      "[Epoch 2/200] [Batch 282/469] [D loss: 0.408796] [G loss: 1.131148]\n",
      "[Epoch 2/200] [Batch 283/469] [D loss: 0.486865] [G loss: 1.153319]\n",
      "[Epoch 2/200] [Batch 284/469] [D loss: 0.583235] [G loss: 0.603312]\n",
      "[Epoch 2/200] [Batch 285/469] [D loss: 0.590750] [G loss: 1.761622]\n",
      "[Epoch 2/200] [Batch 286/469] [D loss: 0.794672] [G loss: 0.284869]\n",
      "[Epoch 2/200] [Batch 287/469] [D loss: 0.611942] [G loss: 2.576042]\n",
      "[Epoch 2/200] [Batch 288/469] [D loss: 0.473149] [G loss: 0.662312]\n",
      "[Epoch 2/200] [Batch 289/469] [D loss: 0.393260] [G loss: 1.424381]\n",
      "[Epoch 2/200] [Batch 290/469] [D loss: 0.350832] [G loss: 1.166546]\n",
      "[Epoch 2/200] [Batch 291/469] [D loss: 0.341633] [G loss: 1.227062]\n",
      "[Epoch 2/200] [Batch 292/469] [D loss: 0.366192] [G loss: 1.387592]\n",
      "[Epoch 2/200] [Batch 293/469] [D loss: 0.379852] [G loss: 0.968500]\n",
      "[Epoch 2/200] [Batch 294/469] [D loss: 0.360540] [G loss: 1.536025]\n",
      "[Epoch 2/200] [Batch 295/469] [D loss: 0.403333] [G loss: 0.863114]\n",
      "[Epoch 2/200] [Batch 296/469] [D loss: 0.423271] [G loss: 1.773300]\n",
      "[Epoch 2/200] [Batch 297/469] [D loss: 0.561396] [G loss: 0.522983]\n",
      "[Epoch 2/200] [Batch 298/469] [D loss: 0.522985] [G loss: 2.447024]\n",
      "[Epoch 2/200] [Batch 299/469] [D loss: 0.639216] [G loss: 0.417444]\n",
      "[Epoch 2/200] [Batch 300/469] [D loss: 0.503449] [G loss: 2.274912]\n",
      "[Epoch 2/200] [Batch 301/469] [D loss: 0.507539] [G loss: 0.563729]\n",
      "[Epoch 2/200] [Batch 302/469] [D loss: 0.375162] [G loss: 1.825804]\n",
      "[Epoch 2/200] [Batch 303/469] [D loss: 0.378368] [G loss: 0.969600]\n",
      "[Epoch 2/200] [Batch 304/469] [D loss: 0.358991] [G loss: 1.187381]\n",
      "[Epoch 2/200] [Batch 305/469] [D loss: 0.402390] [G loss: 1.197865]\n",
      "[Epoch 2/200] [Batch 306/469] [D loss: 0.427732] [G loss: 0.931614]\n",
      "[Epoch 2/200] [Batch 307/469] [D loss: 0.478160] [G loss: 1.263282]\n",
      "[Epoch 2/200] [Batch 308/469] [D loss: 0.532806] [G loss: 0.653162]\n",
      "[Epoch 2/200] [Batch 309/469] [D loss: 0.543905] [G loss: 1.701336]\n",
      "[Epoch 2/200] [Batch 310/469] [D loss: 0.705162] [G loss: 0.350167]\n",
      "[Epoch 2/200] [Batch 311/469] [D loss: 0.692372] [G loss: 2.354425]\n",
      "[Epoch 2/200] [Batch 312/469] [D loss: 0.716380] [G loss: 0.327002]\n",
      "[Epoch 2/200] [Batch 313/469] [D loss: 0.446197] [G loss: 1.992866]\n",
      "[Epoch 2/200] [Batch 314/469] [D loss: 0.398658] [G loss: 0.924007]\n",
      "[Epoch 2/200] [Batch 315/469] [D loss: 0.360749] [G loss: 1.158501]\n",
      "[Epoch 2/200] [Batch 316/469] [D loss: 0.396139] [G loss: 1.397674]\n",
      "[Epoch 2/200] [Batch 317/469] [D loss: 0.441576] [G loss: 0.812998]\n",
      "[Epoch 2/200] [Batch 318/469] [D loss: 0.405387] [G loss: 1.553100]\n",
      "[Epoch 2/200] [Batch 319/469] [D loss: 0.450102] [G loss: 0.822297]\n",
      "[Epoch 2/200] [Batch 320/469] [D loss: 0.396810] [G loss: 1.394212]\n",
      "[Epoch 2/200] [Batch 321/469] [D loss: 0.440485] [G loss: 0.973177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/200] [Batch 322/469] [D loss: 0.435783] [G loss: 1.243188]\n",
      "[Epoch 2/200] [Batch 323/469] [D loss: 0.490595] [G loss: 0.794984]\n",
      "[Epoch 2/200] [Batch 324/469] [D loss: 0.470320] [G loss: 1.476798]\n",
      "[Epoch 2/200] [Batch 325/469] [D loss: 0.572501] [G loss: 0.516588]\n",
      "[Epoch 2/200] [Batch 326/469] [D loss: 0.648591] [G loss: 2.379569]\n",
      "[Epoch 2/200] [Batch 327/469] [D loss: 0.805370] [G loss: 0.244768]\n",
      "[Epoch 2/200] [Batch 328/469] [D loss: 0.539676] [G loss: 2.731040]\n",
      "[Epoch 2/200] [Batch 329/469] [D loss: 0.353869] [G loss: 1.101723]\n",
      "[Epoch 2/200] [Batch 330/469] [D loss: 0.364429] [G loss: 0.919139]\n",
      "[Epoch 2/200] [Batch 331/469] [D loss: 0.417162] [G loss: 1.896034]\n",
      "[Epoch 2/200] [Batch 332/469] [D loss: 0.437015] [G loss: 0.705978]\n",
      "[Epoch 2/200] [Batch 333/469] [D loss: 0.412651] [G loss: 1.753827]\n",
      "[Epoch 2/200] [Batch 334/469] [D loss: 0.438967] [G loss: 0.751614]\n",
      "[Epoch 2/200] [Batch 335/469] [D loss: 0.459768] [G loss: 1.712366]\n",
      "[Epoch 2/200] [Batch 336/469] [D loss: 0.607777] [G loss: 0.469005]\n",
      "[Epoch 2/200] [Batch 337/469] [D loss: 0.809566] [G loss: 2.448019]\n",
      "[Epoch 2/200] [Batch 338/469] [D loss: 0.953592] [G loss: 0.187234]\n",
      "[Epoch 2/200] [Batch 339/469] [D loss: 0.706282] [G loss: 2.111711]\n",
      "[Epoch 2/200] [Batch 340/469] [D loss: 0.562705] [G loss: 0.538262]\n",
      "[Epoch 2/200] [Batch 341/469] [D loss: 0.426330] [G loss: 1.246001]\n",
      "[Epoch 2/200] [Batch 342/469] [D loss: 0.422159] [G loss: 1.210650]\n",
      "[Epoch 2/200] [Batch 343/469] [D loss: 0.469349] [G loss: 0.827835]\n",
      "[Epoch 2/200] [Batch 344/469] [D loss: 0.474654] [G loss: 1.236927]\n",
      "[Epoch 2/200] [Batch 345/469] [D loss: 0.445622] [G loss: 0.844630]\n",
      "[Epoch 2/200] [Batch 346/469] [D loss: 0.428164] [G loss: 1.418883]\n",
      "[Epoch 2/200] [Batch 347/469] [D loss: 0.477505] [G loss: 0.813243]\n",
      "[Epoch 2/200] [Batch 348/469] [D loss: 0.455398] [G loss: 1.251435]\n",
      "[Epoch 2/200] [Batch 349/469] [D loss: 0.473454] [G loss: 0.848973]\n",
      "[Epoch 2/200] [Batch 350/469] [D loss: 0.422469] [G loss: 1.248068]\n",
      "[Epoch 2/200] [Batch 351/469] [D loss: 0.467250] [G loss: 1.002136]\n",
      "[Epoch 2/200] [Batch 352/469] [D loss: 0.458397] [G loss: 0.978571]\n",
      "[Epoch 2/200] [Batch 353/469] [D loss: 0.503612] [G loss: 1.208204]\n",
      "[Epoch 2/200] [Batch 354/469] [D loss: 0.495050] [G loss: 0.637844]\n",
      "[Epoch 2/200] [Batch 355/469] [D loss: 0.526756] [G loss: 2.021509]\n",
      "[Epoch 2/200] [Batch 356/469] [D loss: 0.526813] [G loss: 0.556540]\n",
      "[Epoch 2/200] [Batch 357/469] [D loss: 0.412516] [G loss: 1.810907]\n",
      "[Epoch 2/200] [Batch 358/469] [D loss: 0.373797] [G loss: 0.948037]\n",
      "[Epoch 2/200] [Batch 359/469] [D loss: 0.325127] [G loss: 1.240011]\n",
      "[Epoch 2/200] [Batch 360/469] [D loss: 0.352016] [G loss: 1.410428]\n",
      "[Epoch 2/200] [Batch 361/469] [D loss: 0.395095] [G loss: 0.927671]\n",
      "[Epoch 2/200] [Batch 362/469] [D loss: 0.360254] [G loss: 1.375158]\n",
      "[Epoch 2/200] [Batch 363/469] [D loss: 0.356239] [G loss: 1.014933]\n",
      "[Epoch 2/200] [Batch 364/469] [D loss: 0.392025] [G loss: 1.331290]\n",
      "[Epoch 2/200] [Batch 365/469] [D loss: 0.396260] [G loss: 0.952759]\n",
      "[Epoch 2/200] [Batch 366/469] [D loss: 0.386173] [G loss: 1.338029]\n",
      "[Epoch 2/200] [Batch 367/469] [D loss: 0.361072] [G loss: 0.974813]\n",
      "[Epoch 2/200] [Batch 368/469] [D loss: 0.336430] [G loss: 1.457868]\n",
      "[Epoch 2/200] [Batch 369/469] [D loss: 0.357127] [G loss: 1.143626]\n",
      "[Epoch 2/200] [Batch 370/469] [D loss: 0.358139] [G loss: 1.165259]\n",
      "[Epoch 2/200] [Batch 371/469] [D loss: 0.367660] [G loss: 1.280992]\n",
      "[Epoch 2/200] [Batch 372/469] [D loss: 0.358155] [G loss: 0.984514]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-952185e38119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, 200, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % 10 == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
